
@IEEEtranBSTCTL{IEEEexample:BSTcontrol,
CTLuse_forced_etal       = "yes",
CTLmax_names_forced_etal = "4",
CTLnames_show_etal       = "3" }

@article{aurisanoConvolutionalNeuralNetwork2016,
  title = {A Convolutional Neural Network Neutrino Event Classifier},
  author = {Aurisano, A. and Radovic, A. and Rocco, D. and Himmel, A. and Messier, M.D. and Niner, E. and Pawloski, G. and Psihas, F. and Sousa, A. and Vahle, P.},
  year = {2016},
  month = sep,
  volume = {11},
  pages = {P09001-P09001},
  issn = {1748-0221},
  doi = {10.1088/1748-0221/11/09/P09001},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\HZTZ5PAP\\Aurisano et al. - 2016 - A convolutional neural network neutrino event clas.pdf},
  journal = {Journal of Instrumentation},
  number = {09}
}

@article{backstromSupervisedRandomWalks2010,
  title = {Supervised {{Random Walks}}: {{Predicting}} and {{Recommending Links}} in {{Social Networks}}},
  shorttitle = {Supervised {{Random Walks}}},
  author = {Backstrom, L. and Leskovec, J.},
  year = {2010},
  month = nov,
  abstract = {Predicting the occurrence of links is a fundamental problem in networks. In the link prediction problem we are given a snapshot of a network and would like to infer which interactions among existing members are likely to occur in the near future or which existing interactions are we missing. Although this problem has been extensively studied, the challenge of how to effectively combine the information from the network structure with rich node and edge attribute data remains largely open. We develop an algorithm based on Supervised Random Walks that naturally combines the information from the network structure with node and edge level attributes. We achieve this by using these attributes to guide a random walk on the graph. We formulate a supervised learning task where the goal is to learn a function that assigns strengths to edges in the network such that a random walker is more likely to visit the nodes to which new links will be created in the future. We develop an efficient training algorithm to directly learn the edge strength estimation function. Our experiments on the Facebook social graph and large collaboration networks show that our approach outperforms state-of-the-art unsupervised approaches as well as approaches that are based on feature extraction.},
  archiveprefix = {arXiv},
  eprint = {1011.4071},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\AP7JF5X2\\Backstrom and Leskovec - 2010 - Supervised Random Walks Predicting and Recommendi.pdf;C\:\\Users\\sidch\\Zotero\\storage\\PN9SD8ZH\\1011.html},
  journal = {arXiv:1011.4071 [physics, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Data Structures and Algorithms,Computer Science - Social and Information Networks,Physics - Physics and Society,Statistics - Machine Learning},
  primaryclass = {physics, stat}
}

@article{baronMachineLearningAstronomy2019,
  title = {Machine {{Learning}} in {{Astronomy}}: A Practical Overview},
  shorttitle = {Machine {{Learning}} in {{Astronomy}}},
  author = {Baron, Dalya},
  year = {2019},
  month = apr,
  abstract = {Astronomy is experiencing a rapid growth in data size and complexity. This change fosters the development of data-driven science as a useful companion to the common model-driven data analysis paradigm, where astronomers develop automatic tools to mine datasets and extract novel information from them. In recent years, machine learning algorithms have become increasingly popular among astronomers, and are now used for a wide variety of tasks. In light of these developments, and the promise and challenges associated with them, the IAC Winter School 2018 focused on big data in Astronomy, with a particular emphasis on machine learning and deep learning techniques. This document summarizes the topics of supervised and unsupervised learning algorithms presented during the school, and provides practical information on the application of such tools to astronomical datasets. In this document I cover basic topics in supervised machine learning, including selection and preprocessing of the input dataset, evaluation methods, and three popular supervised learning algorithms, Support Vector Machines, Random Forests, and shallow Artificial Neural Networks. My main focus is on unsupervised machine learning algorithms, that are used to perform cluster analysis, dimensionality reduction, visualization, and outlier detection. Unsupervised learning algorithms are of particular importance to scientific research, since they can be used to extract new knowledge from existing datasets, and can facilitate new discoveries.},
  archiveprefix = {arXiv},
  eprint = {1904.07248},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\XKWVR5A7\\Baron - 2019 - Machine Learning in Astronomy a practical overvie.pdf;C\:\\Users\\sidch\\Zotero\\storage\\DIR3ATAF\\1904.html},
  journal = {arXiv:1904.07248 [astro-ph]},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics},
  primaryclass = {astro-ph}
}

@article{bartokMachineLearningGeneralPurpose2018,
  title = {Machine {{Learning}} a {{General}}-{{Purpose Interatomic Potential}} for {{Silicon}}},
  author = {Bart{\'o}k, Albert P. and Kermode, James and Bernstein, Noam and Cs{\'a}nyi, G{\'a}bor},
  year = {2018},
  month = dec,
  volume = {8},
  pages = {041048},
  issn = {2160-3308},
  doi = {10.1103/PhysRevX.8.041048},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\GNP5VNG4\\Bart√≥k et al. - 2018 - Machine Learning a General-Purpose Interatomic Pot.pdf},
  journal = {Physical Review X},
  language = {en},
  number = {4}
}

@article{bellmZwickyTransientFacility2019,
  title = {The {{Zwicky Transient Facility}}: {{System Overview}}, {{Performance}}, and {{First Results}}},
  shorttitle = {The {{Zwicky Transient Facility}}},
  author = {Bellm, Eric C. and Kulkarni, Shrinivas R. and Graham, Matthew J. and Dekany, Richard and Smith, Roger M. and Riddle, Reed and Masci, Frank J. and Helou, George and Prince, Thomas A. and Adams, Scott M. and Barbarino, C. and Barlow, Tom and Bauer, James and Beck, Ron and Belicki, Justin and Biswas, Rahul and Blagorodnova, Nadejda and Bodewits, Dennis and Bolin, Bryce and Brinnel, Valery and Brooke, Tim and Bue, Brian and Bulla, Mattia and Burruss, Rick and Cenko, S. Bradley and Chang, Chan-Kao and Connolly, Andrew and Coughlin, Michael and Cromer, John and Cunningham, Virginia and De, Kishalay and Delacroix, Alex and Desai, Vandana and Duev, Dmitry A. and Eadie, Gwendolyn and Farnham, Tony L. and Feeney, Michael and Feindt, Ulrich and Flynn, David and Franckowiak, Anna and Frederick, S. and Fremling, C. and {Gal-Yam}, Avishay and Gezari, Suvi and Giomi, Matteo and Goldstein, Daniel A. and Golkhou, V. Zach and Goobar, Ariel and Groom, Steven and Hacopians, Eugean and Hale, David and Henning, John and Ho, Anna Y. Q. and Hover, David and Howell, Justin and Hung, Tiara and Huppenkothen, Daniela and Imel, David and Ip, Wing-Huen and Ivezi{\'c}, {\v Z}eljko and Jackson, Edward and Jones, Lynne and Juric, Mario and Kasliwal, Mansi M. and Kaspi, S. and Kaye, Stephen and Kelley, Michael S. P. and Kowalski, Marek and Kramer, Emily and Kupfer, Thomas and Landry, Walter and Laher, Russ R. and Lee, Chien-De and Lin, Hsing Wen and Lin, Zhong-Yi and Lunnan, Ragnhild and Giomi, Matteo and Mahabal, Ashish and Mao, Peter and Miller, Adam A. and Monkewitz, Serge and Murphy, Patrick and Ngeow, Chow-Choong and Nordin, Jakob and Nugent, Peter and Ofek, Eran and Patterson, Maria T. and Penprase, Bryan and Porter, Michael and Rauch, Ludwig and Rebbapragada, Umaa and Reiley, Dan and Rigault, Mickael and Rodriguez, Hector and {van Roestel}, Jan and Rusholme, Ben and {van Santen}, Jakob and Schulze, S. and Shupe, David L. and Singer, Leo P. and Soumagnac, Maayane T. and Stein, Robert and Surace, Jason and Sollerman, Jesper and Szkody, Paula and Taddia, F. and Terek, Scott and Van Sistine, Angela and {van Velzen}, Sjoert and Vestrand, W. Thomas and Walters, Richard and Ward, Charlotte and Ye, Quan-Zhi and Yu, Po-Chieh and Yan, Lin and Zolkower, Jeffry},
  year = {2019},
  month = jan,
  volume = {131},
  pages = {018002},
  issn = {0004-6280, 1538-3873},
  doi = {10.1088/1538-3873/aaecbe},
  abstract = {The Zwicky Transient Facility (ZTF) is a new optical time-domain survey that uses the Palomar 48-inch Schmidt telescope. A custom-built wide-field camera provides a 47 deg\$\^2\$ field of view and 8 second readout time, yielding more than an order of magnitude improvement in survey speed relative to its predecessor survey, the Palomar Transient Factory (PTF). We describe the design and implementation of the camera and observing system. The ZTF data system at the Infrared Processing and Analysis Center provides near-real-time reduction to identify moving and varying objects. We outline the analysis pipelines, data products, and associated archive. Finally, we present on-sky performance analysis and first scientific results from commissioning and the early survey. ZTF's public alert stream will serve as a useful precursor for that of the Large Synoptic Survey Telescope.},
  archiveprefix = {arXiv},
  eprint = {1902.01932},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\UF42MM4B\\Bellm et al. - 2019 - The Zwicky Transient Facility System Overview, Pe.pdf;C\:\\Users\\sidch\\Zotero\\storage\\PZ5CFICI\\1902.html},
  journal = {Publications of the Astronomical Society of the Pacific},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics},
  number = {995}
}

@article{booneAvocadoPhotometricClassification2019,
  title = {Avocado: {{Photometric Classification}} of {{Astronomical Transients}} with {{Gaussian Process Augmentation}}},
  shorttitle = {Avocado},
  author = {Boone, Kyle},
  year = {2019},
  month = jul,
  doi = {10.3847/1538-3881/ab5182},
  abstract = {Upcoming astronomical surveys such as the Large Synoptic Survey Telescope (LSST) will rely on photometric classification to identify the majority of the transients and variables that they discover. We present a set of techniques for photometric classification that can be applied even when the training set of spectroscopically-confirmed objects is heavily biased towards bright, low-redshift objects. Using Gaussian process regression to model arbitrary light curves in all bands simultaneously, we "augment" the training set by generating new versions of the original light curves covering a range of redshifts and observing conditions. We train a boosted decision tree classifier on features extracted from the augmented light curves, and we show how such a classifier can be designed to produce classifications that are independent of the redshift distributions of objects in the training sample. Our classification algorithm was the best-performing among the 1,094 models considered in the blinded phase of the Photometric LSST Astronomical Time-Series Classification Challenge (PLAsTiCC), scoring 0.468 on the organizers' logarithmic-loss metric with flat weights for all object classes in the training set, and achieving an AUC of 0.957 for classification of Type Ia supernovae. Our results suggest that spectroscopic campaigns used for training photometric classifiers should focus on typing large numbers of well-observed, intermediate redshift transients instead of attempting to type a sample of transients that is directly representative of the full dataset being classified. All of the algorithms described in this paper are implemented in the avocado software package.},
  archiveprefix = {arXiv},
  eprint = {1907.04690},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\52WPETNA\\Boone - 2019 - Avocado Photometric Classification of Astronomica.pdf;C\:\\Users\\sidch\\Zotero\\storage\\AEX96A6R\\1907.html},
  journal = {arXiv:1907.04690 [astro-ph]},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics},
  primaryclass = {astro-ph}
}

@article{carrasco-davisAlertClassificationALeRCE2020,
  title = {Alert {{Classification}} for the {{ALeRCE Broker System}}: {{The Real}}-Time {{Stamp Classifier}}},
  shorttitle = {Alert {{Classification}} for the {{ALeRCE Broker System}}},
  author = {{Carrasco-Davis}, Rodrigo and Reyes, Esteban and Valenzuela, Camilo and F{\"o}rster, Francisco and Est{\'e}vez, Pablo A. and Pignata, Giuliano and Bauer, Franz E. and Reyes, Ignacio and {S{\'a}nchez-S{\'a}ez}, Paula and {Cabrera-Vives}, Guillermo and Eyheramendy, Susana and Catelan, M{\'a}rcio and Arredondo, Javier and {Castillo-Navarrete}, Ernesto and {Rodr{\'i}guez-Mancini}, Diego and {Ruz-Mieres}, Daniela and Moya, Alberto and {Sabatini-Gacit{\'u}a}, Luis and {Sep{\'u}lveda-Cobo}, Crist{\'o}bal and Mahabal, Ashish A. and {Silva-Farf{\'a}n}, Javier and {Camacho-I{\~n}iquez}, Ernesto and Galbany, Llu{\'i}s},
  year = {2020},
  month = aug,
  abstract = {We present a real-time stamp classifier of astronomical events for the ALeRCE (Automatic Learning for the Rapid Classification of Events) broker. The classifier is based on a convolutional neural network with an architecture designed to exploit rotational invariance of the images, and trained on alerts ingested from the Zwicky Transient Facility (ZTF). Using only the \textbackslash textit\{science, reference\} and \textbackslash textit\{difference\} images of the first detection as inputs, along with the metadata of the alert as features, the classifier is able to correctly classify alerts from active galactic nuclei, supernovae (SNe), variable stars, asteroids and bogus classes, with high accuracy (\$\textbackslash sim\$94\textbackslash\%) in a balanced test set. In order to find and analyze SN candidates selected by our classifier from the ZTF alert stream, we designed and deployed a visualization tool called SN Hunter, where relevant information about each possible SN is displayed for the experts to choose among candidates to report to the Transient Name Server database. We have reported 3060 SN candidates to date (9.2 candidates per day on average), of which 394 have been confirmed spectroscopically. Our ability to report objects using only a single detection means that 92\textbackslash\% of the reported SNe occurred within one day after the first detection. ALeRCE has only reported candidates not otherwise detected or selected by other groups, therefore adding new early transients to the bulk of objects available for early follow-up. Our work represents an important milestone toward rapid alert classifications with the next generation of large etendue telescopes, such as the Vera C. Rubin Observatory's Legacy Survey of Space and Time.},
  archiveprefix = {arXiv},
  eprint = {2008.03309},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\EVYBB2RS\\Carrasco-Davis et al. - 2020 - Alert Classification for the ALeRCE Broker System.pdf;C\:\\Users\\sidch\\Zotero\\storage\\NDVKUD55\\2008.html},
  journal = {arXiv:2008.03309 [astro-ph]},
  keywords = {Astrophysics - High Energy Astrophysical Phenomena,Astrophysics - Instrumentation and Methods for Astrophysics,Computer Science - Machine Learning},
  primaryclass = {astro-ph}
}

@article{chainiAstronomicalClassificationLight2020,
  title = {Astronomical {{Classification}} of {{Light Curves}} with an {{Ensemble}} of {{Gated Recurrent Units}}},
  author = {Chaini, Siddharth and Kumar, Soumya Sanjay},
  year = {2020},
  month = jul,
  abstract = {With an ever-increasing amount of astronomical data being collected, manual classification has become obsolete; and machine learning is the only way forward. Keeping this in mind, the Large Synoptic Survey Telescope (LSST) Team hosted the Photometric LSST Astronomical Time-Series Classification Challenge (PLAsTiCC) in 2018. The aim of this challenge was to develop models that accurately classify astronomical sources into different classes, scaling from a limited training set to a large test set. In this text, we report our results of experimenting with Bidirectional Gated Recurrent Unit (GRU) based deep learning models to deal with time series data of the PLAsTiCC dataset. We demonstrate that GRUs are indeed suitable to handle time series data. With minimum preprocessing and without augmentation, our stacked ensemble of GRU and Dense networks achieves an accuracy of 76.243\%. Data from astronomical surveys such as LSST will help researchers answer questions pertaining to dark matter, dark energy and the origins of the universe; accurate classification of astronomical sources is the first step towards achieving this. Our code is open-source and has been made available on GitHub here: https://github.com/AKnightWing/Astronomical-Classification-PLASTICC},
  archiveprefix = {arXiv},
  eprint = {2006.12333},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\XDLNANVW\\Chaini and Kumar - 2020 - Astronomical Classification of Light Curves with a.pdf;C\:\\Users\\sidch\\Zotero\\storage\\NJM4WDCV\\2006.html},
  journal = {arXiv:2006.12333 [astro-ph]},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics},
  primaryclass = {astro-ph}
}

@article{chengIdentifyingStrongLenses2020,
  title = {Identifying {{Strong Lenses}} with {{Unsupervised Machine Learning}} Using {{Convolutional Autoencoder}}},
  author = {Cheng, Ting-Yun and Li, Nan and Conselice, Christopher J. and {Arag{\'o}n-Salamanca}, Alfonso and Dye, Simon and Metcalf, Robert B.},
  year = {2020},
  month = may,
  volume = {494},
  pages = {3750--3765},
  issn = {0035-8711, 1365-2966},
  doi = {10.1093/mnras/staa1015},
  abstract = {In this paper we develop a new unsupervised machine learning technique comprised of a feature extractor, a convolutional autoencoder (CAE), and a clustering algorithm consisting of a Bayesian Gaussian mixture model (BGM). We apply this technique to visual band space-based simulated imaging data from the Euclid Space Telescope using data from the Strong Gravitational Lenses Finding Challenge. Our technique promisingly captures a variety of lensing features such as Einstein rings with different radii, distorted arc structures, etc, without using predefined labels. After the clustering process, we obtain several classification clusters separated by different visual features which are seen in the images. Our method successfully picks up \$\textbackslash sim\$63\textbackslash{} percent of lensing images from all lenses in the training set. With the assumed probability proposed in this study, this technique reaches an accuracy of \$77.25\textbackslash pm 0.48\$\textbackslash\% in binary classification using the training set. Additionally, our unsupervised clustering process can be used as the preliminary classification for future surveys of lenses to efficiently select targets and to speed up the labelling process. As the starting point of the astronomical application using this technique, we not only explore the application to gravitationally lensed systems, but also discuss the limitations and potential future uses of this technique.},
  archiveprefix = {arXiv},
  eprint = {1911.04320},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\7GMJII5Y\\Cheng et al. - 2020 - Identifying Strong Lenses with Unsupervised Machin.pdf;C\:\\Users\\sidch\\Zotero\\storage\\W3C85VJ2\\1911.html},
  journal = {Monthly Notices of the Royal Astronomical Society},
  keywords = {Astrophysics - Astrophysics of Galaxies,Astrophysics - Cosmology and Nongalactic Astrophysics,Astrophysics - Instrumentation and Methods for Astrophysics},
  number = {3}
}

@article{coughlinZTFSourceClassification2020,
  title = {The {{ZTF Source Classification Project}}: {{II}}. {{Periodicity}} and Variability Processing Metrics},
  shorttitle = {The {{ZTF Source Classification Project}}},
  author = {Coughlin, Michael W. and Burdge, Kevin and Duev, Dmitry A. and Katz, Michael L. and {van Roestel}, Jan and Drake, Andrew and Graham, Matthew J. and Hillenbrand, Lynne and Mahabal, Ashish A. and Masci, Frank J. and Mr{\'o}z, Przemek and Prince, Thomas A. and Yao, Yuhan and Bellm, Eric C. and Burruss, Rick and Dekany, Richard and Jaodand, Amruta and Kaplan, David L. and Kupfer, Thomas and Laher, Russ R. and Riddle, Reed and Rigault, Mickael and Rodriguez, Hector and Rusholme, Ben and Zolkower, Jeffry},
  year = {2020},
  month = sep,
  abstract = {The current generation of all-sky surveys is rapidly expanding our ability to study variable and transient sources. These surveys, with a variety of sensitivities, cadences, and fields of view, probe many ranges of timescale and magnitude. Data from the Zwicky Transient Facility (ZTF) yields an opportunity to find variables on timescales from minutes to months. In this paper, we present the codebase, ztfperiodic, and the computational metrics employed for the catalogue based on ZTF's Second Data Release. We describe the publicly available, graphical-process-unit optimized period-finding algorithms employed, and highlight the benefit of existing and future graphical-process-unit clusters. We show how generating metrics as input to catalogues of this scale is possible for future ZTF data releases. Further work will be needed for future data from the Vera C. Rubin Observatory's Legacy Survey of Space and Time.},
  archiveprefix = {arXiv},
  eprint = {2009.14071},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\QVVBJHBM\\Coughlin et al. - 2020 - The ZTF Source Classification Project II. Periodi.pdf;C\:\\Users\\sidch\\Zotero\\storage\\MKXJRYWG\\2009.html},
  journal = {arXiv:2009.14071 [astro-ph]},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics,Astrophysics - Solar and Stellar Astrophysics},
  primaryclass = {astro-ph}
}

@article{daiPhotometricClassificationRedshift2018,
  title = {Photometric Classification and Redshift Estimation of {{LSST Supernovae}}},
  author = {Dai, Mi and Kuhlmann, Steve and Wang, Yun and Kovacs, Eve},
  year = {2018},
  month = apr,
  doi = {10.1093/mnras/sty965},
  abstract = {Supernova (SN) classification and redshift estimation using photometric data only have become very important for the Large Synoptic Survey Telescope (LSST), given the large number of SNe that LSST will observe and the impossibility of spectroscopically following up all the SNe. We investigate the performance of a SN classifier that uses SN colors to classify LSST SNe with the Random Forest classification algorithm. Our classifier results in an AUC of 0.98 which represents excellent classification. We are able to obtain a photometric SN sample containing 99\$\textbackslash\%\$ SNe Ia by choosing a probability threshold. We estimate the photometric redshifts (photo-z) of SNe in our sample by fitting the SN light curves using the SALT2 model with nested sampling. We obtain a mean bias (\$\textbackslash left{$<$}z\_\textbackslash mathrm\{phot\}-z\_\textbackslash mathrm\{spec\}\textbackslash right{$>\$$}) of 0.012 with \$\textbackslash sigma\textbackslash left( \textbackslash frac\{z\_\textbackslash mathrm\{phot\}-z\_\textbackslash mathrm\{spec\}\}\{1+z\_\textbackslash mathrm\{spec\}\}\textbackslash right) = 0.0294\$ without using a host-galaxy photo-z prior, and a mean bias (\$\textbackslash left{$<$}z\_\textbackslash mathrm\{phot\}-z\_\textbackslash mathrm\{spec\}\textbackslash right{$>\$$}) of 0.0017 with \$\textbackslash sigma\textbackslash left( \textbackslash frac\{z\_\textbackslash mathrm\{phot\}-z\_\textbackslash mathrm\{spec\}\}\{1+z\_\textbackslash mathrm\{spec\}\}\textbackslash right) = 0.0116\$ using a host-galaxy photo-z prior. Assuming a flat \$\textbackslash Lambda CDM\$ model with \$\textbackslash Omega\_m=0.3\$, we obtain \$\textbackslash Omega\_m\$ of \$0.305\textbackslash pm0.008\$ (statistical errors only), using the simulated LSST sample of photometric SNe Ia (with intrinsic scatter \$\textbackslash sigma\_\textbackslash mathrm\{int\}=0.11\$) derived using our methodology without using host-galaxy photo-z prior. Our method will help boost the power of SNe from the LSST as cosmological probes.},
  archiveprefix = {arXiv},
  eprint = {1701.05689},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\AHR57NB4\\Dai et al. - 2018 - Photometric classification and redshift estimation.pdf;C\:\\Users\\sidch\\Zotero\\storage\\3MIXY5V7\\1701.html},
  journal = {arXiv:1701.05689 [astro-ph]},
  keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics,Astrophysics - Instrumentation and Methods for Astrophysics},
  primaryclass = {astro-ph}
}

@article{deoliveiraJetImagesDeepLearning2016,
  title = {Jet-{{Images}} -- {{Deep Learning Edition}}},
  author = {{de Oliveira}, Luke and Kagan, Michael and Mackey, Lester and Nachman, Benjamin and Schwartzman, Ariel},
  year = {2016},
  month = jul,
  volume = {2016},
  pages = {69},
  issn = {1029-8479},
  doi = {10.1007/JHEP07(2016)069},
  abstract = {Building on the notion of a particle physics detector as a camera and the collimated streams of high energy particles, or jets, it measures as an image, we investigate the potential of machine learning techniques based on deep learning architectures to identify highly boosted W bosons. Modern deep learning algorithms trained on jet images can out-perform standard physically-motivated feature driven approaches to jet tagging. We develop techniques for visualizing how these features are learned by the network and what additional information is used to improve performance. This interplay between physically-motivated feature driven tools and supervised learning algorithms is general and can be used to significantly increase the sensitivity to discover new particles and new forces, and gain a deeper understanding of the physics within jets.},
  archiveprefix = {arXiv},
  eprint = {1511.05190},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\PH9Z3CYU\\de Oliveira et al. - 2016 - Jet-Images -- Deep Learning Edition.pdf;C\:\\Users\\sidch\\Zotero\\storage\\26ESWA9K\\1511.html},
  journal = {Journal of High Energy Physics},
  keywords = {High Energy Physics - Phenomenology,Physics - Data Analysis; Statistics and Probability,Statistics - Machine Learning},
  number = {7}
}

@book{dezaEncyclopediaDistances2013,
  title = {Encyclopedia of {{Distances}}},
  author = {Deza, Michel Marie and Deza, Elena},
  year = {2013},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-30958-8},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\UF4RCVCF\\Deza and Deza - 2013 - Encyclopedia of Distances.pdf},
  isbn = {978-3-642-30957-1 978-3-642-30958-8},
  language = {en}
}

@article{disantoPhotometricRedshiftEstimation2017,
  title = {Photometric Redshift Estimation via Deep Learning},
  author = {D'Isanto, Antonio and Polsterer, Kai Lars},
  year = {2017},
  month = sep,
  doi = {10.1051/0004-6361/201731326},
  abstract = {The need to analyze the available large synoptic multi-band surveys drives the development of new data-analysis methods. Photometric redshift estimation is one field of application where such new methods improved the results, substantially. Up to now, the vast majority of applied redshift estimation methods have utilized photometric features. We aim to develop a method to derive probabilistic photometric redshift directly from multi-band imaging data, rendering pre-classification of objects and feature extraction obsolete. A modified version of a deep convolutional network was combined with a mixture density network. The estimates are expressed as Gaussian mixture models representing the probability density functions (PDFs) in the redshift space. In addition to the traditional scores, the continuous ranked probability score (CRPS) and the probability integral transform (PIT) were applied as performance criteria. We have adopted a feature based random forest and a plain mixture density network to compare performances on experiments with data from SDSS (DR9). We show that the proposed method is able to predict redshift PDFs independently from the type of source, for example galaxies, quasars or stars. Thereby the prediction performance is better than both presented reference methods and is comparable to results from the literature. The presented method is extremely general and allows us to solve of any kind of probabilistic regression problems based on imaging data, for example estimating metallicity or star formation rate of galaxies. This kind of methodology is tremendously important for the next generation of surveys.},
  archiveprefix = {arXiv},
  eprint = {1706.02467},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\DJI2G2MA\\D'Isanto and Polsterer - 2017 - Photometric redshift estimation via deep learning.pdf;C\:\\Users\\sidch\\Zotero\\storage\\PK2Z5F84\\1706.html},
  journal = {arXiv:1706.02467 [astro-ph]},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics},
  primaryclass = {astro-ph}
}

@article{estebanRealvaluedMedicalTime2017,
  title = {Real-Valued ({{Medical}}) {{Time Series Generation}} with {{Recurrent Conditional GANs}}},
  author = {Esteban, Crist{\'o}bal and Hyland, Stephanie L. and R{\"a}tsch, Gunnar},
  year = {2017},
  month = dec,
  abstract = {Generative Adversarial Networks (GANs) have shown remarkable success as a framework for training models to produce realistic-looking data. In this work, we propose a Recurrent GAN (RGAN) and Recurrent Conditional GAN (RCGAN) to produce realistic real-valued multi-dimensional time series, with an emphasis on their application to medical data. RGANs make use of recurrent neural networks in the generator and the discriminator. In the case of RCGANs, both of these RNNs are conditioned on auxiliary information. We demonstrate our models in a set of toy datasets, where we show visually and quantitatively (using sample likelihood and maximum mean discrepancy) that they can successfully generate realistic time-series. We also describe novel evaluation methods for GANs, where we generate a synthetic labelled training dataset, and evaluate on a real test set the performance of a model trained on the synthetic data, and vice-versa. We illustrate with these metrics that RCGANs can generate time-series data useful for supervised training, with only minor degradation in performance on real test data. This is demonstrated on digit classification from 'serialised' MNIST and by training an early warning system on a medical dataset of 17,000 patients from an intensive care unit. We further discuss and analyse the privacy concerns that may arise when using RCGANs to generate realistic synthetic medical time series data.},
  archiveprefix = {arXiv},
  eprint = {1706.02633},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\4BE4A589\\Esteban et al. - 2017 - Real-valued (Medical) Time Series Generation with .pdf;C\:\\Users\\sidch\\Zotero\\storage\\LXGMDG25\\1706.html},
  journal = {arXiv:1706.02633 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@article{forsterAutomaticLearningRapid2020,
  title = {The {{Automatic Learning}} for the {{Rapid Classification}} of {{Events}} ({{ALeRCE}}) {{Alert Broker}}},
  author = {F{\"o}rster, F. and {Cabrera-Vives}, G. and {Castillo-Navarrete}, E. and Est{\'e}vez, P. A. and {S{\'a}nchez-S{\'a}ez}, P. and Arredondo, J. and Bauer, F. E. and {Carrasco-Davis}, R. and Catelan, M. and Elorrieta, F. and Eyheramendy, S. and Huijse, P. and Pignata, G. and Reyes, E. and Reyes, I. and {Rodr{\'i}guez-Mancini}, D. and {Ruz-Mieres}, D. and Valenzuela, C. and {Alvarez-Maldonado}, I. and Astorga, N. and Borissova, J. and Clocchiatti, A. and De Cicco, D. and {Donoso-Oliva}, C. and Graham, M. J. and Kurtev, R. and Mahabal, A. and Maureira, J. C. and {Molina-Ferreiro}, R. and Moya, A. and Palma, W. and {P{\'e}rez-Carrasco}, M. and Protopapas, P. and Romero, M. and {Sabatini-Gacit{\'u}a}, L. and S{\'a}nchez, A. and Mart{\'i}n, J. San and {Sep{\'u}lveda-Cobo}, C. and Vera, E. and Vergara, J. R.},
  year = {2020},
  month = aug,
  abstract = {We introduce the Automatic Learning for the Rapid Classification of Events (ALeRCE) broker, an astronomical alert broker designed to provide a rapid and self--consistent classification of large etendue telescope alert streams, such as that provided by the Zwicky Transient Facility (ZTF) and, in the future, the Vera C. Rubin Observatory Legacy Survey of Space and Time (LSST). ALeRCE is a Chilean--led broker run by an interdisciplinary team of astronomers and engineers, working to become intermediaries between survey and follow--up facilities. ALeRCE uses a pipeline which includes the real--time ingestion, aggregation, cross--matching, machine learning (ML) classification, and visualization of the ZTF alert stream. We use two classifiers: a stamp--based classifier, designed for rapid classification, and a light--curve--based classifier, which uses the multi--band flux evolution to achieve a more refined classification. We describe in detail our pipeline, data products, tools and services, which are made public for the community (see \textbackslash url\{https://alerce.science\}). Since we began operating our real--time ML classification of the ZTF alert stream in early 2019, we have grown a large community of active users around the globe. We describe our results to date, including the real--time processing of \$9.7\textbackslash times10\^7\$ alerts, the stamp classification of \$1.9\textbackslash times10\^7\$ objects, the light curve classification of \$8.5\textbackslash times10\^5\$ objects, the report of 3088 supernova candidates, and different experiments using LSST-like alert streams. Finally, we discuss the challenges ahead to go from a single-stream of alerts such as ZTF to a multi--stream ecosystem dominated by LSST.},
  archiveprefix = {arXiv},
  eprint = {2008.03303},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\BZSL33XI\\F√∂rster et al. - 2020 - The Automatic Learning for the Rapid Classificatio.pdf;C\:\\Users\\sidch\\Zotero\\storage\\ZJ5RNG8T\\2008.html},
  journal = {arXiv:2008.03303 [astro-ph]},
  keywords = {Astrophysics - High Energy Astrophysical Phenomena,Astrophysics - Instrumentation and Methods for Astrophysics,Astrophysics - Solar and Stellar Astrophysics},
  primaryclass = {astro-ph}
}

@article{gasteggerMachineLearningMolecular2017,
  title = {Machine Learning Molecular Dynamics for the Simulation of Infrared Spectra},
  author = {Gastegger, Michael and Behler, J{\"o}rg and Marquetand, Philipp},
  year = {2017},
  volume = {8},
  pages = {6924--6935},
  issn = {2041-6520, 2041-6539},
  doi = {10.1039/C7SC02267K},
  abstract = {Artificial neural networks are combined with molecular dynamics to simulate molecular infrared spectra including anharmonicities and temperature effects.           ,                             Machine learning has emerged as an invaluable tool in many research areas. In the present work, we harness this power to predict highly accurate molecular infrared spectra with unprecedented computational efficiency. To account for vibrational anharmonic and dynamical effects \textendash{} typically neglected by conventional quantum chemistry approaches \textendash{} we base our machine learning strategy on               ab initio               molecular dynamics simulations. While these simulations are usually extremely time consuming even for small molecules, we overcome these limitations by leveraging the power of a variety of machine learning techniques, not only accelerating simulations by several orders of magnitude, but also greatly extending the size of systems that can be treated. To this end, we develop a molecular dipole moment model based on environment dependent neural network charges and combine it with the neural network potential approach of Behler and Parrinello. Contrary to the prevalent big data philosophy, we are able to obtain very accurate machine learning models for the prediction of infrared spectra based on only a few hundreds of electronic structure reference points. This is made possible through the use of molecular forces during neural network potential training and the introduction of a fully automated sampling scheme. We demonstrate the power of our machine learning approach by applying it to model the infrared spectra of a methanol molecule,               n               -alkanes containing up to 200 atoms and the protonated alanine tripeptide, which at the same time represents the first application of machine learning techniques to simulate the dynamics of a peptide. In all of these case studies we find an excellent agreement between the infrared spectra predicted               via               machine learning models and the respective theoretical and experimental spectra.},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\SRNFAQY8\\Gastegger et al. - 2017 - Machine learning molecular dynamics for the simula.pdf},
  journal = {Chemical Science},
  language = {en},
  number = {10}
}

@article{georgeDeepLearningRealtime2018,
  title = {Deep {{Learning}} for {{Real}}-Time {{Gravitational Wave Detection}} and {{Parameter Estimation}}: {{Results}} with {{Advanced LIGO Data}}},
  shorttitle = {Deep {{Learning}} for {{Real}}-Time {{Gravitational Wave Detection}} and {{Parameter Estimation}}},
  author = {George, Daniel and Huerta, E. A.},
  year = {2018},
  month = mar,
  volume = {778},
  pages = {64--70},
  issn = {03702693},
  doi = {10.1016/j.physletb.2017.12.053},
  abstract = {The recent Nobel-prize-winning detections of gravitational waves from merging black holes and the subsequent detection of the collision of two neutron stars in coincidence with electromagnetic observations have inaugurated a new era of multimessenger astrophysics. To enhance the scope of this emergent field of science, we pioneered the use of deep learning with convolutional neural networks, that take time-series inputs, for rapid detection and characterization of gravitational wave signals. This approach, Deep Filtering, was initially demonstrated using simulated LIGO noise. In this article, we present the extension of Deep Filtering using real data from LIGO, for both detection and parameter estimation of gravitational waves from binary black hole mergers using continuous data streams from multiple LIGO detectors. We demonstrate for the first time that machine learning can detect and estimate the true parameters of real events observed by LIGO. Our results show that Deep Filtering achieves similar sensitivities and lower errors compared to matched-filtering while being far more computationally efficient and more resilient to glitches, allowing real-time processing of weak time-series signals in non-stationary non-Gaussian noise with minimal resources, and also enables the detection of new classes of gravitational wave sources that may go unnoticed with existing detection algorithms. This unified framework for data analysis is ideally suited to enable coincident detection campaigns of gravitational waves and their multimessenger counterparts in real-time.},
  archiveprefix = {arXiv},
  eprint = {1711.03121},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\LYE22K2J\\George and Huerta - 2018 - Deep Learning for Real-time Gravitational Wave Det.pdf;C\:\\Users\\sidch\\Zotero\\storage\\QNQQAN89\\1711.html},
  journal = {Physics Letters B},
  keywords = {Astrophysics - High Energy Astrophysical Phenomena,Astrophysics - Instrumentation and Methods for Astrophysics,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,General Relativity and Quantum Cosmology}
}

@article{gligorovEfficientReliableFast2012,
  title = {Efficient, Reliable and Fast High-Level Triggering Using a Bonsai Boosted Decision Tree},
  author = {Gligorov, Vladimir Vava and Williams, Mike},
  year = {2012},
  month = oct,
  doi = {10.1088/1748-0221/8/02/P02013},
  abstract = {High-level triggering is a vital component in many modern particle physics experiments. This paper describes a modification to the standard boosted decision tree (BDT) classifier, the so-called "bonsai" BDT, that has the following important properties: it is more efficient than traditional cut-based approaches; it is robust against detector instabilities, and it is very fast. Thus, it is fit-for-purpose for the online running conditions faced by any large-scale data acquisition system.},
  archiveprefix = {arXiv},
  eprint = {1210.6861},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\268ABD3T\\Gligorov and Williams - 2012 - Efficient, reliable and fast high-level triggering.pdf;C\:\\Users\\sidch\\Zotero\\storage\\4MD2M2YW\\1210.html},
  journal = {arXiv:1210.6861 [hep-ex, physics:physics]},
  keywords = {High Energy Physics - Experiment,Physics - Data Analysis; Statistics and Probability,Physics - Instrumentation and Detectors},
  primaryclass = {hep-ex, physics:physics}
}

@article{grahamZwickyTransientFacility2019,
  title = {The {{Zwicky Transient Facility}}: {{Science Objectives}}},
  shorttitle = {The {{Zwicky Transient Facility}}},
  author = {Graham, Matthew J. and Kulkarni, S. R. and Bellm, Eric C. and Adams, Scott M. and Barbarino, Cristina and Blagorodnova, Nadejda and Bodewits, Dennis and Bolin, Bryce and Brady, Patrick R. and Cenko, S. Bradley and Chang, Chan-Kao and Coughlin, Michael W. and De, Kishalay and Eadie, Gwendolyn and Farnham, Tony L. and Feindt, Ulrich and Franckowiak, Anna and Fremling, Christoffer and {Gal-yam}, Avishay and Gezari, Suvi and Ghosh, Shaon and Goldstein, Daniel A. and Golkhou, V. Zach and Goobar, Ariel and Ho, Anna Y. Q. and Huppenkothen, Daniela and Ivezic, Zeljko and Jones, R. Lynne and Juric, Mario and Kaplan, David L. and Kasliwal, Mansi M. and Kelley, Michael S. P. and Kupfer, Thomas and Lee, Chien-De and Lin, Hsing Wen and Lunnan, Ragnhild and Mahabal, Ashish A. and Miller, Adam A. and Ngeow, Chow-Choong and Nugent, Peter and Ofek, Eran O. and Prince, Thomas A. and Rauch, Ludwig and {van Roestel}, Jan and Schulze, Steve and Singer, Leo P. and Sollerman, Jesper and Taddia, Francesco and Yan, Lin and Ye, Quan-Zhi and Yu, Po-Chieh and Andreoni, Igor and Barlow, Tom and Bauer, James and Beck, Ron and Belicki, Justin and Biswas, Rahul and Brinnel, Valery and Brooke, Tim and Bue, Brian and Bulla, Mattia and Burdge, Kevin and Burruss, Rick and Connolly, Andrew and Cromer, John and Cunningham, Virginia and Dekany, Richard and Delacroix, Alex and Desai, Vandana and Duev, Dmitry A. and Hacopians, Eugean and Hale, David and Helou, George and Henning, John and Hover, David and Hillenbrand, Lynne A. and Howell, Justin and Hung, Tiara and Imel, David and Ip, Wing-Huen and Jackson, Edward and Kaspi, Shai and Kaye, Stephen and Kowalski, Marek and Kramer, Emily and Kuhn, Michael and Landry, Walter and Laher, Russ R. and Mao, Peter and Masci, Frank J. and Monkewitz, Serge and Murphy, Patrick and Nordin, Jakob and Patterson, Maria T. and Penprase, Bryan and Porter, Michael and Rebbapragada, Umaa and Reiley, Dan and Riddle, Reed and Rigault, Mickael and Rodriguez, Hector and Rusholme, Ben and {van Santen}, Jakob and Shupe, David L. and Smith, Roger M. and Soumagnac, Maayane T. and Stein, Robert and Surace, Jason and Szkody, Paula and Terek, Scott and {van Sistine}, Angela and {van Velzen}, Sjoert and Vestrand, W. Thomas and Walters, Richard and Ward, Charlotte and Zhang, Chaoran and Zolkower, Jeffry},
  year = {2019},
  month = jul,
  volume = {131},
  pages = {078001},
  issn = {0004-6280, 1538-3873},
  doi = {10.1088/1538-3873/ab006c},
  abstract = {The Zwicky Transient Facility (ZTF), a public-private enterprise, is a new time domain survey employing a dedicated camera on the Palomar 48-inch Schmidt telescope with a 47 deg\$\^2\$ field of view and 8 second readout time. It is well positioned in the development of time domain astronomy, offering operations at 10\% of the scale and style of the Large Synoptic Survey Telescope (LSST) with a single 1-m class survey telescope. The public surveys will cover the observable northern sky every three nights in g and r filters and the visible Galactic plane every night in g and r. Alerts generated by these surveys are sent in real time to brokers. A consortium of universities which provided funding ("partnership") are undertaking several boutique surveys. The combination of these surveys producing one million alerts per night allows for exploration of transient and variable astrophysical phenomena brighter than r \$\textbackslash sim\$ 20.5 on timescales of minutes to years. We describe the primary science objectives driving ZTF including the physics of supernovae and relativistic explosions, multi-messenger astrophysics, supernova cosmology, active galactic nuclei and tidal disruption events, stellar variability, and Solar System objects.},
  archiveprefix = {arXiv},
  eprint = {1902.01945},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\4SPTX375\\Graham et al. - 2019 - The Zwicky Transient Facility Science Objectives.pdf;C\:\\Users\\sidch\\Zotero\\storage\\2V22C2GL\\1902.html},
  journal = {Publications of the Astronomical Society of the Pacific},
  keywords = {Astrophysics - High Energy Astrophysical Phenomena,Astrophysics - Instrumentation and Methods for Astrophysics},
  number = {1001}
}

@article{guestJetFlavorClassification2016,
  title = {Jet {{Flavor Classification}} in {{High}}-{{Energy Physics}} with {{Deep Neural Networks}}},
  author = {Guest, Daniel and Collado, Julian and Baldi, Pierre and Hsu, Shih-Chieh and Urban, Gregor and Whiteson, Daniel},
  year = {2016},
  month = sep,
  doi = {10.1103/PhysRevD.94.112002},
  abstract = {Classification of jets as originating from light-flavor or heavy-flavor quarks is an important task for inferring the nature of particles produced in high-energy collisions. The large and variable dimensionality of the data provided by the tracking detectors makes this task difficult. The current state-of-the-art tools require expert data-reduction to convert the data into a fixed low-dimensional form that can be effectively managed by shallow classifiers. We study the application of deep networks to this task, attempting classification at several levels of data, starting from a raw list of tracks. We find that the highest-level lowest-dimensionality expert information sacrifices information needed for classification, that the performance of current state-of-the-art taggers can be matched or slightly exceeded by deep-network-based taggers using only track and vertex information, that classification using only lowest-level highest-dimensionality tracking information remains a difficult task for deep networks, and that adding lower-level track and vertex information to the classifiers provides a significant boost in performance compared to the state-of-the-art.},
  archiveprefix = {arXiv},
  eprint = {1607.08633},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\AF2Q23L5\\Guest et al. - 2016 - Jet Flavor Classification in High-Energy Physics w.pdf;C\:\\Users\\sidch\\Zotero\\storage\\T982K9LT\\1607.html},
  journal = {arXiv:1607.08633 [hep-ex, physics:physics]},
  keywords = {High Energy Physics - Experiment,Physics - Data Analysis; Statistics and Probability},
  primaryclass = {hep-ex, physics:physics}
}

@article{heDeepResidualLearning2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = dec,
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archiveprefix = {arXiv},
  eprint = {1512.03385},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\C2XKZW5C\\He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf;C\:\\Users\\sidch\\Zotero\\storage\\8W9ZKFAG\\1512.html},
  journal = {arXiv:1512.03385 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@article{hinnersMachineLearningTechniques2018,
  title = {Machine {{Learning Techniques}} for {{Stellar Light Curve Classification}}},
  author = {Hinners, Trisha and Tat, Kevin and Thorp, Rachel},
  year = {2018},
  month = jun,
  volume = {156},
  pages = {7},
  issn = {1538-3881},
  doi = {10.3847/1538-3881/aac16d},
  abstract = {We apply machine learning techniques in an attempt to predict and classify stellar properties from noisy and sparse time series data. We preprocessed over 94 GB of Kepler light curves from MAST to classify according to ten distinct physical properties using both representation learning and feature engineering approaches. Studies using machine learning in the field have been primarily done on simulated data, making our study one of the first to use real light curve data for machine learning approaches. We tuned our data using previous work with simulated data as a template and achieved mixed results between the two approaches. Representation learning using a Long Short-Term Memory (LSTM) Recurrent Neural Network (RNN) produced no successful predictions, but our work with feature engineering was successful for both classification and regression. In particular, we were able to achieve values for stellar density, stellar radius, and effective temperature with low error (\textasciitilde{} 2 - 4\%) and good accuracy (\textasciitilde{} 75\%) for classifying the number of transits for a given star. The results show promise for improvement for both approaches upon using larger datasets with a larger minority class. This work has the potential to provide a foundation for future tools and techniques to aid in the analysis of astrophysical data.},
  archiveprefix = {arXiv},
  eprint = {1710.06804},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\2TF9S78W\\Hinners et al. - 2018 - Machine Learning Techniques for Stellar Light Curv.pdf;C\:\\Users\\sidch\\Zotero\\storage\\XXTATC78\\1710.html},
  journal = {The Astronomical Journal},
  keywords = {85,Astrophysics - Instrumentation and Methods for Astrophysics},
  number = {1}
}

@article{kennamerActiveLearningRESSPECT2020a,
  title = {Active Learning with {{RESSPECT}}: {{Resource}} Allocation for Extragalactic Astronomical Transients},
  shorttitle = {Active Learning with {{RESSPECT}}},
  author = {Kennamer, Noble and Ishida, Emille E. O. and {Gonzalez-Gaitan}, Santiago and {de Souza}, Rafael S. and Ihler, Alexander and Ponder, Kara and Vilalta, Ricardo and Moller, Anais and Jones, David O. and Dai, Mi and {Krone-Martins}, Alberto and Quint, Bruno and Sreejith, Sreevarsha and Malz, Alex I. and Galbany, Lluis},
  year = {2020},
  month = oct,
  abstract = {The recent increase in volume and complexity of available astronomical data has led to a wide use of supervised machine learning techniques. Active learning strategies have been proposed as an alternative to optimize the distribution of scarce labeling resources. However, due to the specific conditions in which labels can be acquired, fundamental assumptions, such as sample representativeness and labeling cost stability cannot be fulfilled. The Recommendation System for Spectroscopic follow-up (RESSPECT) project aims to enable the construction of optimized training samples for the Rubin Observatory Legacy Survey of Space and Time (LSST), taking into account a realistic description of the astronomical data environment. In this work, we test the robustness of active learning techniques in a realistic simulated astronomical data scenario. Our experiment takes into account the evolution of training and pool samples, different costs per object, and two different sources of budget. Results show that traditional active learning strategies significantly outperform random sampling. Nevertheless, more complex batch strategies are not able to significantly overcome simple uncertainty sampling techniques. Our findings illustrate three important points: 1) active learning strategies are a powerful tool to optimize the label-acquisition task in astronomy, 2) for upcoming large surveys like LSST, such techniques allow us to tailor the construction of the training sample for the first day of the survey, and 3) the peculiar data environment related to the detection of astronomical transients is a fertile ground that calls for the development of tailored machine learning algorithms.},
  archiveprefix = {arXiv},
  eprint = {2010.05941},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\AN3R45BC\\Kennamer et al. - 2020 - Active learning with RESSPECT Resource allocation.pdf;C\:\\Users\\sidch\\Zotero\\storage\\6XXPD3RF\\2010.html},
  journal = {arXiv:2010.05941 [astro-ph]},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics,Computer Science - Artificial Intelligence},
  primaryclass = {astro-ph}
}

@article{kimStargalaxyClassificationUsing2017,
  title = {Star-Galaxy {{Classification Using Deep Convolutional Neural Networks}}},
  author = {Kim, Edward J. and Brunner, Robert J.},
  year = {2017},
  month = feb,
  volume = {464},
  pages = {4463--4475},
  issn = {0035-8711, 1365-2966},
  doi = {10.1093/mnras/stw2672},
  abstract = {Most existing star-galaxy classifiers use the reduced summary information from catalogs, requiring careful feature extraction and selection. The latest advances in machine learning that use deep convolutional neural networks allow a machine to automatically learn the features directly from data, minimizing the need for input from human experts. We present a star-galaxy classification framework that uses deep convolutional neural networks (ConvNets) directly on the reduced, calibrated pixel values. Using data from the Sloan Digital Sky Survey (SDSS) and the Canada-France-Hawaii Telescope Lensing Survey (CFHTLenS), we demonstrate that ConvNets are able to produce accurate and well-calibrated probabilistic classifications that are competitive with conventional machine learning techniques. Future advances in deep learning may bring more success with current and forthcoming photometric surveys, such as the Dark Energy Survey (DES) and the Large Synoptic Survey Telescope (LSST), because deep neural networks require very little, manual feature engineering.},
  archiveprefix = {arXiv},
  eprint = {1608.04369},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\2BBD6HZI\\Kim and Brunner - 2017 - Star-galaxy Classification Using Deep Convolutiona.pdf;C\:\\Users\\sidch\\Zotero\\storage\\ZSPJ54UF\\1608.html},
  journal = {Monthly Notices of the Royal Astronomical Society},
  keywords = {Astrophysics - Astrophysics of Galaxies,Astrophysics - Cosmology and Nongalactic Astrophysics,Astrophysics - Instrumentation and Methods for Astrophysics,Computer Science - Computer Vision and Pattern Recognition},
  number = {4}
}

@article{korsunskyInferenceCancerProgression2014,
  title = {Inference of {{Cancer Progression Models}} with {{Biological Noise}}},
  author = {Korsunsky, Ilya and Ramazzotti, Daniele and Caravagna, Giulio and Mishra, Bud},
  year = {2014},
  month = aug,
  abstract = {Many applications in translational medicine require the understanding of how diseases progress through the accumulation of persistent events. Specialized Bayesian networks called monotonic progression networks offer a statistical framework for modeling this sort of phenomenon. Current machine learning tools to reconstruct Bayesian networks from data are powerful but not suited to progression models. We combine the technological advances in machine learning with a rigorous philosophical theory of causation to produce Polaris, a scalable algorithm for learning progression networks that accounts for causal or biological noise as well as logical relations among genetic events, making the resulting models easy to interpret qualitatively. We tested Polaris on synthetically generated data and showed that it outperforms a widely used machine learning algorithm and approaches the performance of the competing special-purpose, albeit clairvoyant algorithm that is given a priori information about the model parameters. We also prove that under certain rather mild conditions, Polaris is guaranteed to converge for sufficiently large sample sizes. Finally, we applied Polaris to point mutation and copy number variation data in Prostate cancer from The Cancer Genome Atlas (TCGA) and found that there are likely three distinct progressions, one major androgen driven progression, one major non-androgen driven progression, and one novel minor androgen driven progression.},
  archiveprefix = {arXiv},
  eprint = {1408.6032},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\37US8N5S\\Korsunsky et al. - 2014 - Inference of Cancer Progression Models with Biolog.pdf;C\:\\Users\\sidch\\Zotero\\storage\\LY77P6X2\\1408.html},
  journal = {arXiv:1408.6032 [cs, q-bio, stat]},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods,Statistics - Machine Learning},
  primaryclass = {cs, q-bio, stat}
}

@article{krizhevskyImageNetClassificationDeep2017,
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  year = {2017},
  month = may,
  volume = {60},
  pages = {84--90},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3065386},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\5DWIVKBR\\Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf},
  journal = {Communications of the ACM},
  language = {en},
  number = {6}
}

@article{liptonLearningDiagnoseLSTM2017,
  title = {Learning to {{Diagnose}} with {{LSTM Recurrent Neural Networks}}},
  author = {Lipton, Zachary C. and Kale, David C. and Elkan, Charles and Wetzel, Randall},
  year = {2017},
  month = mar,
  abstract = {Clinical medical data, especially in the intensive care unit (ICU), consist of multivariate time series of observations. For each patient visit (or episode), sensor data and lab test results are recorded in the patient's Electronic Health Record (EHR). While potentially containing a wealth of insights, the data is difficult to mine effectively, owing to varying length, irregular sampling and missing data. Recurrent Neural Networks (RNNs), particularly those using Long Short-Term Memory (LSTM) hidden units, are powerful and increasingly popular models for learning from sequence data. They effectively model varying length sequences and capture long range dependencies. We present the first study to empirically evaluate the ability of LSTMs to recognize patterns in multivariate time series of clinical measurements. Specifically, we consider multilabel classification of diagnoses, training a model to classify 128 diagnoses given 13 frequently but irregularly sampled clinical measurements. First, we establish the effectiveness of a simple LSTM network for modeling clinical data. Then we demonstrate a straightforward and effective training strategy in which we replicate targets at each sequence step. Trained only on raw time series, our models outperform several strong baselines, including a multilayer perceptron trained on hand-engineered features.},
  archiveprefix = {arXiv},
  eprint = {1511.03677},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\K5IKCDMU\\Lipton et al. - 2017 - Learning to Diagnose with LSTM Recurrent Neural Ne.pdf;C\:\\Users\\sidch\\Zotero\\storage\\N9Z3G9KN\\1511.html},
  journal = {arXiv:1511.03677 [cs]},
  keywords = {Computer Science - Machine Learning},
  primaryclass = {cs}
}

@article{liuSentimentAnalysisOpinion2012,
  title = {Sentiment {{Analysis}} and {{Opinion Mining}}},
  author = {Liu, Bing},
  year = {2012},
  month = may,
  volume = {5},
  pages = {1--167},
  issn = {1947-4040, 1947-4059},
  doi = {10.2200/S00416ED1V01Y201204HLT016},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\YQ44WTEA\\Liu - 2012 - Sentiment Analysis and Opinion Mining.pdf},
  journal = {Synthesis Lectures on Human Language Technologies},
  language = {en},
  number = {1}
}

@misc{long-gangpangTrainingTestingData2017,
  title = {Training and Testing Data Used in the Paper "{{An}} Equation-of-State-Meter of {{QCD}} Transition from Deep Learning"},
  author = {{Long-Gang Pang} and Zhou, Kai and Su, Nan and Petersen, Hannah and Stocker, Horst and {Xin-Nian Wang}},
  year = {2017},
  pages = {337941479 Bytes},
  publisher = {{figshare}},
  doi = {10.6084/M9.FIGSHARE.5457220.V1},
  abstract = {Training and testing data to identify the QCD transition using deep learning and traditional machine learning.{$<$}br{$>$}1. training\_data.csv, testing\_iebevishnu.csv, testing\_ipglasma.csv{$<$}br{$>$}There are 723 entries in each row. The first row is the description of the data. The 0th entry in each row is the event id, the entries from 1 to 720 are the pion density distribution at mid-rapidity -- rho(pt, phi) at 15 different pt bins and 48 different azimuthal angle phi bins with phi as the inner loop. The 721st entry is the equation of state type (0 or 1). The 722 entry is extra information of each event.{$<$}br{$>$}2. training\_observables.csv, test\_iebe\_observables.csv, test\_ipglasma\_observables.csv {$<$}br{$>$}There are 87 entries in each row. The first row is the header which describes the data of the following rows. In the following rows, the first entry is the event id, the second entry is the equation of state type (0 or 1), the remaining entries are 85 observables computed from raw spectra, which will be used in various traditional classifiers in machine learning toolbox.{$<$}br{$>$}The meaning of these entries and the Monte Carlo model used to generate these data can be found in the paper,http://inspirehep.net/record/1503189{$<$}br{$><$}br{$><$}br{$>$}},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {20202 Nuclear Physics,20203 Particle Physics,20303 Fluid Physics,29902 Complex Physical Systems,Computational Physics,FOS: Physical sciences}
}

@article{mahabalDeepLearntClassificationLight2017,
  title = {Deep-{{Learnt Classification}} of {{Light Curves}}},
  author = {Mahabal, Ashish and Sheth, Kshiteej and Gieseke, Fabian and Pai, Akshay and Djorgovski, S. George and Drake, Andrew and Graham, Matthew and Collaboration, the CSS/CRTS/PTF},
  year = {2017},
  month = nov,
  pages = {1--8},
  doi = {10.1109/SSCI.2017.8280984},
  abstract = {Astronomy light curves are sparse, gappy, and heteroscedastic. As a result standard time series methods regularly used for financial and similar datasets are of little help and astronomers are usually left to their own instruments and techniques to classify light curves. A common approach is to derive statistical features from the time series and to use machine learning methods, generally supervised, to separate objects into a few of the standard classes. In this work, we transform the time series to two-dimensional light curve representations in order to classify them using modern deep learning techniques. In particular, we show that convolutional neural networks based classifiers work well for broad characterization and classification. We use labeled datasets of periodic variables from CRTS survey and show how this opens doors for a quick classification of diverse classes with several possible exciting extensions.},
  archiveprefix = {arXiv},
  eprint = {1709.06257},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\I6FDMCYJ\\Mahabal et al. - 2017 - Deep-Learnt Classification of Light Curves.pdf;C\:\\Users\\sidch\\Zotero\\storage\\H4WG2XUX\\1709.html},
  journal = {2017 IEEE Symposium Series on Computational Intelligence (SSCI)},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics,Computer Science - Computer Vision and Pattern Recognition}
}

@article{mahabalMachineLearningZwicky2019,
  title = {Machine {{Learning}} for the {{Zwicky Transient Facility}}},
  author = {Mahabal, Ashish and Rebbapragada, Umaa and Walters, Richard and Masci, Frank J. and Blagorodnova, Nadejda and {van Roestel}, Jan and Ye, Quan-Zhi and Biswas, Rahul and Burdge, Kevin and Chang, Chan-Kao and Duev, Dmitry A. and Golkhou, V. Zach and Miller, Adam A. and Nordin, Jakob and Ward, Charlotte and Adams, Scott and Bellm, Eric C. and Branton, Doug and Bue, Brian and Cannella, Chris and Connolly, Andrew and Dekany, Richard and Feindt, Ulrich and Hung, Tiara and Fortson, Lucy and Frederick, Sara and Fremling, C. and Gezari, Suvi and Graham, Matthew and Groom, Steven and Kasliwal, Mansi M. and Kulkarni, Shrinivas and Kupfer, Thomas and Lin, Hsing Wen and Lintott, Chris and Lunnan, Ragnhild and Parejko, John and Prince, Thomas A. and Riddle, Reed and Rusholme, Ben and Saunders, Nicholas and Sedaghat, Nima and Shupe, David L. and Singer, Leo P. and Soumagnac, Maayane T. and Szkody, Paula and Tachibana, Yutaro and Tirumala, Kushal and {van Velzen}, Sjoert and Wright, Darryl},
  year = {2019},
  month = mar,
  volume = {131},
  pages = {038002},
  issn = {0004-6280, 1538-3873},
  doi = {10.1088/1538-3873/aaf3fa},
  abstract = {The Zwicky Transient Facility is a large optical survey in multiple filters producing hundreds of thousands of transient alerts per night. We describe here various machine learning (ML) implementations and plans to make the maximal use of the large data set by taking advantage of the temporal nature of the data, and further combining it with other data sets. We start with the initial steps of separating bogus candidates from real ones, separating stars and galaxies, and go on to the classification of real objects into various classes. Besides the usual methods (e.g., based on features extracted from light curves) we also describe early plans for alternate methods including the use of domain adaptation, and deep learning. In a similar fashion we describe efforts to detect fast moving asteroids. We also describe the use of the Zooniverse platform for helping with classifications through the creation of training samples, and active learning. Finally we mention the synergistic aspects of ZTF and LSST from the ML perspective.},
  archiveprefix = {arXiv},
  eprint = {1902.01936},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\Z6BY8AMK\\Mahabal et al. - 2019 - Machine Learning for the Zwicky Transient Facility.pdf;C\:\\Users\\sidch\\Zotero\\storage\\3T3JYQKZ\\1902.html},
  journal = {Publications of the Astronomical Society of the Pacific},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics},
  number = {997}
}

@article{masciZwickyTransientFacility2019,
  title = {The {{Zwicky Transient Facility}}: {{Data Processing}}, {{Products}}, and {{Archive}}},
  shorttitle = {The {{Zwicky Transient Facility}}},
  author = {Masci, Frank J. and Laher, Russ R. and Rusholme, Ben and Shupe, David L. and Groom, Steven and Surace, Jason and Jackson, Edward and Monkewitz, Serge and Beck, Ron and Flynn, David and Terek, Scott and Landry, Walter and Hacopians, Eugean and Desai, Vandana and Howell, Justin and Brooke, Tim and Imel, David and Wachter, Stefanie and Ye, Quan-Zhi and Lin, Hsing-Wen and Cenko, S. Bradley and Cunningham, Virginia and Rebbapragada, Umaa and Bue, Brian and Miller, Adam A. and Mahabal, Ashish and Bellm, Eric C. and Patterson, Maria T. and Juri{\'c}, Mario and Golkhou, V. Zach and Ofek, Eran O. and Walters, Richard and Graham, Matthew and Kasliwal, Mansi M. and Dekany, Richard G. and Kupfer, Thomas and Burdge, Kevin and Cannella, Christopher B. and Barlow, Tom and Van Sistine, Angela and Giomi, Matteo and Fremling, Christoffer and Blagorodnova, Nadejda and Levitan, David and Riddle, Reed and Smith, Roger M. and Helou, George and Prince, Thomas A. and Kulkarni, Shrinivas R.},
  year = {2019},
  month = jan,
  volume = {131},
  pages = {018003},
  issn = {0004-6280, 1538-3873},
  doi = {10.1088/1538-3873/aae8ac},
  abstract = {The Zwicky Transient Facility (ZTF) is a new robotic time-domain survey currently in progress using the Palomar 48-inch Schmidt Telescope. ZTF uses a 47 square degree field with a 600 megapixel camera to scan the entire northern visible sky at rates of \textasciitilde 3760 square degrees/hour to median depths of g \textasciitilde{} 20.8 and r \textasciitilde{} 20.6 mag (AB, 5sigma in 30 sec). We describe the Science Data System that is housed at IPAC, Caltech. This comprises the data-processing pipelines, alert production system, data archive, and user interfaces for accessing and analyzing the products. The realtime pipeline employs a novel image-differencing algorithm, optimized for the detection of point source transient events. These events are vetted for reliability using a machine-learned classifier and combined with contextual information to generate data-rich alert packets. The packets become available for distribution typically within 13 minutes (95th percentile) of observation. Detected events are also linked to generate candidate moving-object tracks using a novel algorithm. Objects that move fast enough to streak in the individual exposures are also extracted and vetted. The reconstructed astrometric accuracy per science image with respect to Gaia is typically 45 to 85 milliarcsec. This is the RMS per axis on the sky for sources extracted with photometric S/N {$>$}= 10. The derived photometric precision (repeatability) at bright unsaturated fluxes varies between 8 and 25 millimag. Photometric calibration accuracy with respect to Pan-STARRS1 is generally better than 2\%. The products support a broad range of scientific applications: fast and young supernovae, rare flux transients, variable stars, eclipsing binaries, variability from active galactic nuclei, counterparts to gravitational wave sources, a more complete census of Type Ia supernovae, and Solar System objects.},
  archiveprefix = {arXiv},
  eprint = {1902.01872},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\CV5R5ZC2\\Masci et al. - 2019 - The Zwicky Transient Facility Data Processing, Pr.pdf;C\:\\Users\\sidch\\Zotero\\storage\\KSTRNNLH\\1902.html},
  journal = {Publications of the Astronomical Society of the Pacific},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics},
  number = {995}
}

@article{mikolovDistributedRepresentationsWords2013,
  title = {Distributed {{Representations}} of {{Words}} and {{Phrases}} and Their {{Compositionality}}},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  month = oct,
  abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
  archiveprefix = {arXiv},
  eprint = {1310.4546},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\4J6ES3EM\\Mikolov et al. - 2013 - Distributed Representations of Words and Phrases a.pdf;C\:\\Users\\sidch\\Zotero\\storage\\GCPMCBHM\\1310.html},
  journal = {arXiv:1310.4546 [cs, stat]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@article{parameswaranOptimalCrowdpoweredRating2014,
  title = {Optimal Crowd-Powered Rating and Filtering Algorithms},
  author = {Parameswaran, Aditya and Boyd, Stephen and {Garcia-Molina}, Hector and Gupta, Ashish and Polyzotis, Neoklis and Widom, Jennifer},
  year = {2014},
  month = may,
  volume = {7},
  pages = {685--696},
  issn = {2150-8097},
  doi = {10.14778/2732939.2732942},
  abstract = {We focus on crowd-powered filtering, i.e., filtering a large set of items using humans. Filtering is one of the most commonly used building blocks in crowdsourcing applications and systems. While solutions for crowd-powered filtering exist, they make a range of implicit assumptions and restrictions, ultimately rendering them not powerful enough for real-world applications. We describe two approaches to discard these implicit assumptions and restrictions: one, that carefully generalizes prior work, leading to an optimal, but often-times intractable solution, and another, that provides a novel way of reasoning about filtering strategies, leading to a sometimes suboptimal, but efficiently computable solution (that is asymptotically close to optimal). We demonstrate that our techniques lead to significant reductions in error of up to 30\% for fixed cost over prior work in a novel crowdsourcing application: peer evaluation in online courses.},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\VK9ND72H\\Parameswaran et al. - 2014 - Optimal crowd-powered rating and filtering algorit.pdf},
  journal = {Proceedings of the VLDB Endowment},
  language = {en},
  number = {9}
}

@article{pasquetPELICANDeePArchitecturE2019,
  title = {{{PELICAN}}: {{deeP architecturE}} for the {{LIght Curve ANalysis}}},
  shorttitle = {{{PELICAN}}},
  author = {Pasquet, Johanna and Pasquet, J{\'e}r{\^o}me and Chaumont, Marc and Fouchez, Dominique},
  year = {2019},
  month = jul,
  journal = {Astronomy \& Astrophysics},
  volume = {627},
  pages = {A21},
  issn = {0004-6361, 1432-0746},
  doi = {10.1051/0004-6361/201834473},
  abstract = {We developed a deeP architecturE for the LIght Curve ANalysis (PELICAN) for the characterization and the classification of supernovae light curves. It takes light curves as input, without any additional features. PELICAN can deal with the sparsity and the irregular sampling of light curves. It is designed to remove the problem of non-representativeness between the training and test databases coming from the limitations of the spectroscopic follow-up. We applied our methodology on different supernovae light curve databases. First, we tested PELICAN on the Supernova Photometric Classification Challenge for which we obtained the best performance ever achieved with a non-representative training database, by reaching an accuracy of 0.811. Then we tested PELICAN on simulated light curves of the LSST Deep Fields for which PELICAN is able to detect 87.4\% of supernovae Ia with a precision higher than 98\%, by considering a non-representative training database of 2k light curves. PELICAN can be trained on light curves of LSST Deep Fields to classify light curves of the LSST main survey, which have a lower sampling rate and are more noisy. In this scenario, it reaches an accuracy of 96.5\% with a training database of 2k light curves of the Deep Fields. This constitutes a pivotal result as type Ia supernovae candidates from the main survey might then be used to increase the statistics without additional spectroscopic follow-up. Finally we tested PELICAN on real data from the Sloan Digital Sky Survey. PELICAN reaches an accuracy of 86.8\% with a training database composed of simulated data and a fraction of 10\% of real data. The ability of PELICAN to deal with the different causes of non-representativeness between the training and test databases, and its robustness against survey properties and observational conditions, put it in the forefront of light curve classification tools for the LSST era.},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\M4LS5N8D\\Pasquet et al. - 2019 - PELICAN deeP architecturE for the LIght Curve ANa.pdf}
}

@article{pasquetPhotometricRedshiftsSDSS2019,
  title = {Photometric Redshifts from {{SDSS}} Images Using a {{Convolutional Neural Network}}},
  author = {Pasquet, Johanna and Bertin, Emmanuel and Treyer, Marie and Arnouts, St{\'e}phane and Fouchez, Dominique},
  year = {2019},
  month = jan,
  volume = {621},
  pages = {A26},
  issn = {0004-6361, 1432-0746},
  doi = {10.1051/0004-6361/201833617},
  abstract = {We developed a Deep Convolutional Neural Network (CNN), used as a classifier, to estimate photometric redshifts and associated probability distribution functions (PDF) for galaxies in the Main Galaxy Sample of the Sloan Digital Sky Survey at z {$<$} 0.4. Our method exploits all the information present in the images without any feature extraction. The input data consist of 64x64 pixel ugriz images centered on the spectroscopic targets, plus the galactic reddening value on the line-of-sight. For training sets of 100k objects or more (\$\textbackslash geq\$ 20\% of the database), we reach a dispersion \$\textbackslash sigma\_\{MAD\}\${$<$}0.01, significantly lower than the current best one obtained from another machine learning technique on the same sample. The bias is lower than 0.0001, independent of photometric redshift. The PDFs are shown to have very good predictive power. We also find that the CNN redshifts are unbiased with respect to galaxy inclination, and that \$\textbackslash sigma\_\{MAD\}\$ decreases with the signal-to-noise ratio (SNR), achieving values below 0.007 for SNR {$>$}100, as in the deep stacked region of Stripe 82. We argue that for most galaxies the precision is limited by the SNR of SDSS images rather than by the method. The success of this experiment at low redshift opens promising perspectives for upcoming surveys.},
  archiveprefix = {arXiv},
  eprint = {1806.06607},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\8CLE8P2V\\Pasquet et al. - 2019 - Photometric redshifts from SDSS images using a Con.pdf;C\:\\Users\\sidch\\Zotero\\storage\\EZBJLWAS\\1806.html},
  journal = {Astronomy \& Astrophysics},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics}
}

@inproceedings{penningtonGloveGlobalVectors2014,
  title = {Glove: {{Global Vectors}} for {{Word Representation}}},
  shorttitle = {Glove},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  year = {2014},
  pages = {1532--1543},
  publisher = {{Association for Computational Linguistics}},
  address = {{Doha, Qatar}},
  doi = {10.3115/v1/D14-1162},
  language = {en}
}

@article{perleyZwickyTransientFacility2020,
  title = {The {{Zwicky Transient Facility Bright Transient Survey}}. {{II}}. {{A Public Statistical Sample}} for {{Exploring Supernova Demographics}}},
  author = {Perley, Daniel A. and Fremling, Christoffer and Sollerman, Jesper and Miller, Adam A. and Dahiwale, Aishwarya S. and Sharma, Yashvi and Bellm, Eric C. and Biswas, Rahul and Brink, Thomas G. and Bruch, Rachel J. and De, Kishalay and Dekany, Richard and Drake, Andrew J. and Duev, Dmitry A. and Filippenko, Alexei V. and {Gal-Yam}, Avishay and Goobar, Ariel and Graham, Matthew J. and Graham, Melissa L. and Ho, Anna Y. Q. and Irani, Ido and Kasliwal, Mansi M. and Kim, Young-Lo and Kulkarni, S. R. and Mahabal, Ashish and Masci, Frank J. and Modak, Shaunak and Neill, James D. and Nordin, Jakob and Riddle, Reed L. and Soumagnac, Maayane T. and Strotjohann, Nora L. and Schulze, Steve and Taggart, Kirsty and Tzanidakis, Anastasios and Walters, Richard S. and Yan, Lin},
  year = {2020},
  month = nov,
  volume = {904},
  pages = {35},
  issn = {1538-4357},
  doi = {10.3847/1538-4357/abbd98},
  abstract = {We present a public catalog of transients from the Zwicky Transient Facility (ZTF) Bright Transient Survey (BTS), a magnitude-limited (m{$<$}19 mag in either the g or r filter) survey for extragalactic transients in the ZTF public stream. We introduce cuts on survey coverage, sky visibility around peak light, and other properties unconnected to the nature of the transient, and show that the resulting statistical sample is spectroscopically 97\% complete at {$<$}18 mag, 93\% complete at {$<$}18.5 mag, and 75\% complete at {$<$}19 mag. We summarize the fundamental properties of this population, identifying distinct duration-luminosity correlations in a variety of supernova (SN) classes and associating the majority of fast optical transients with well-established spectroscopic SN types (primarily SN Ibn and II/IIb). We measure the Type Ia SN and core-collapse (CC) SN rates and luminosity functions, which show good consistency with recent work. About 7\% of CC SNe explode in very low-luminosity galaxies (M\_i {$>$} -16 mag), 10\% in red-sequence galaxies, and 1\% in massive ellipticals. We find no significant difference in the luminosity or color distributions between the host galaxies of Type II and Type Ib/c supernovae, suggesting that line-driven wind stripping does not play a major role in the loss of the hydrogen envelope from their progenitors. Future large-scale classification efforts with ZTF and other wide-area surveys will provide high-quality measurements of the rates, properties, and environments of all known types of optical transients and limits on the existence of theoretically predicted but as of yet unobserved explosions.},
  archiveprefix = {arXiv},
  eprint = {2009.01242},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\IH2ZHB9A\\Perley et al. - 2020 - The Zwicky Transient Facility Bright Transient Sur.pdf;C\:\\Users\\sidch\\Zotero\\storage\\HK5FEX6J\\2009.html},
  journal = {The Astrophysical Journal},
  keywords = {Astrophysics - High Energy Astrophysical Phenomena},
  number = {1}
}

@article{raniAnalysisHeartDiseases2011,
  title = {Analysis of {{Heart Diseases Dataset}} Using {{Neural Network Approach}}},
  author = {Rani, K. Usha},
  year = {2011},
  month = sep,
  volume = {1},
  pages = {1--8},
  issn = {2231007X},
  doi = {10.5121/ijdkp.2011.1501},
  abstract = {One of the important techniques of Data mining is Classification. Many real world problems in various fields such as business, science, industry and medicine can be solved by using classification approach. Neural Networks have emerged as an important tool for classification. The advantages of Neural Networks helps for efficient classification of given data. In this study a Heart diseases dataset is analyzed using Neural Network approach. To increase the efficiency of the classification process parallel approach is also adopted in the training phase.},
  archiveprefix = {arXiv},
  eprint = {1110.2626},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\3IHEDE4U\\Rani - 2011 - Analysis of Heart Diseases Dataset using Neural Ne.pdf;C\:\\Users\\sidch\\Zotero\\storage\\TYKUF4KY\\1110.html},
  journal = {International Journal of Data Mining \& Knowledge Management Process},
  keywords = {Computer Science - Databases,Computer Science - Machine Learning},
  number = {5}
}

@article{ruppFastAccurateModeling2011,
  title = {Fast and {{Accurate Modeling}} of {{Molecular Atomization Energies}} with {{Machine Learning}}},
  author = {Rupp, Matthias and Tkatchenko, Alexandre and M{\"u}ller, Klaus-Robert and {von Lilienfeld}, O. Anatole},
  year = {2011},
  month = sep,
  doi = {10.1103/PhysRevLett.108.058301},
  abstract = {We introduce a machine learning model to predict atomization energies of a diverse set of organic molecules, based on nuclear charges and atomic positions only. The problem of solving the molecular Schr\textbackslash "odinger equation is mapped onto a non-linear statistical regression problem of reduced complexity. Regression models are trained on and compared to atomization energies computed with hybrid density-functional theory. Cross-validation over more than seven thousand small organic molecules yields a mean absolute error of \textasciitilde 10 kcal/mol. Applicability is demonstrated for the prediction of molecular atomization potential energy curves.},
  archiveprefix = {arXiv},
  eprint = {1109.2618},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\8ZIDV6DG\\Rupp et al. - 2011 - Fast and Accurate Modeling of Molecular Atomizatio.pdf;C\:\\Users\\sidch\\Zotero\\storage\\QRHHDZ2K\\1109.html},
  journal = {arXiv:1109.2618 [cond-mat, physics:physics, stat]},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Condensed Matter - Materials Science,Physics - Chemical Physics,Statistics - Machine Learning},
  primaryclass = {cond-mat, physics:physics, stat}
}

@article{shanahanMachineLearningAction2018,
  title = {Machine Learning Action Parameters in Lattice Quantum Chromodynamics},
  author = {Shanahan, Phiala E. and Trewartha, Daniel and Detmold, William},
  year = {2018},
  month = may,
  volume = {97},
  pages = {094506},
  issn = {2470-0010, 2470-0029},
  doi = {10.1103/PhysRevD.97.094506},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\Z3UEND39\\Shanahan et al. - 2018 - Machine learning action parameters in lattice quan.pdf},
  journal = {Physical Review D},
  language = {en},
  number = {9}
}

@article{simonyanVeryDeepConvolutional2015,
  title = {Very {{Deep Convolutional Networks}} for {{Large}}-{{Scale Image Recognition}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  year = {2015},
  month = apr,
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  archiveprefix = {arXiv},
  eprint = {1409.1556},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\PBAMPQDD\\Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf;C\:\\Users\\sidch\\Zotero\\storage\\R3VITKYA\\1409.html},
  journal = {arXiv:1409.1556 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@article{smithANI1ExtensibleNeural2017,
  title = {{{ANI}}-1: An Extensible Neural Network Potential with {{DFT}} Accuracy at Force Field Computational Cost},
  shorttitle = {{{ANI}}-1},
  author = {Smith, J. S. and Isayev, O. and Roitberg, A. E.},
  year = {2017},
  volume = {8},
  pages = {3192--3203},
  issn = {2041-6520, 2041-6539},
  doi = {10.1039/C6SC05720A},
  abstract = {We demonstrate how a deep neural network (NN) trained on a data set of quantum mechanical (QM) DFT calculated energies can learn an accurate and transferable atomistic potential for organic molecules containing H, C, N, and O atoms.           ,              Deep learning is revolutionizing many areas of science and technology, especially image, text, and speech recognition. In this paper, we demonstrate how a deep neural network (NN) trained on quantum mechanical (QM) DFT calculations can learn an accurate and transferable potential for organic molecules. We introduce ANAKIN-ME (Accurate NeurAl networK engINe for Molecular Energies) or ANI for short. ANI is a new method designed with the intent of developing transferable neural network potentials that utilize a highly-modified version of the Behler and Parrinello symmetry functions to build single-atom atomic environment vectors (AEV) as a molecular representation. AEVs provide the ability to train neural networks to data that spans both configurational and conformational space, a feat not previously accomplished on this scale. We utilized ANI to build a potential called ANI-1, which was trained on a subset of the GDB databases with up to 8 heavy atoms in order to predict total energies for organic molecules containing four atom types: H, C, N, and O. To obtain an accelerated but physically relevant sampling of molecular potential surfaces, we also proposed a Normal Mode Sampling (NMS) method for generating molecular conformations. Through a series of case studies, we show that ANI-1 is chemically accurate compared to reference DFT calculations on much larger molecular systems (up to 54 atoms) than those included in the training data set.},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\C79CTWMN\\Smith et al. - 2017 - ANI-1 an extensible neural network potential with.pdf},
  journal = {Chemical Science},
  language = {en},
  number = {4}
}

@article{stetsonAutomaticDeterminationLightCurve1996,
  title = {On the {{Automatic Determination}} of {{Light}}-{{Curve Parameters}} for {{Cepheid Variables}}},
  author = {Stetson, Peter B.},
  year = {1996},
  month = oct,
  volume = {108},
  pages = {851},
  issn = {0004-6280, 1538-3873},
  doi = {10.1086/133808},
  journal = {Publications of the Astronomical Society of the Pacific},
  language = {en}
}

@article{tarcaMachineLearningIts2007,
  title = {Machine {{Learning}} and {{Its Applications}} to {{Biology}}},
  author = {Tarca, Adi L and Carey, Vincent J and Chen, Xue-wen and Romero, Roberto and Dr{\u a}ghici, Sorin},
  editor = {Lewitter, Fran},
  year = {2007},
  month = jun,
  volume = {3},
  pages = {e116},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.0030116},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\ZYWDHBXW\\Tarca et al. - 2007 - Machine Learning and Its Applications to Biology.pdf},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {6}
}

@article{tsarisHEPTrkXProject2018,
  title = {The {{HEP}}.{{TrkX Project}}: {{Deep Learning}} for {{Particle Tracking}}},
  shorttitle = {The {{HEP}}.{{TrkX Project}}},
  author = {Tsaris, Aristeidis and Anderson, Dustin and Bendavid, Josh and Calafiura, Paolo and Cerati, Giuseppe and Esseiva, Julien and Farrell, Steven and Gray, Lindsey and Kapoor, Keshav and Kowalkowski, Jim and Mudigonda, Mayur and {Prabhat} and Spentzouris, Panagiotis and Spiropoulou, Maria and Vlimant, Jean-Roch and Zheng, Stephan and Zurawski, Daniel},
  year = {2018},
  month = sep,
  volume = {1085},
  pages = {042023},
  issn = {1742-6588, 1742-6596},
  doi = {10.1088/1742-6596/1085/4/042023},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\BX5IWU3H\\Tsaris et al. - 2018 - The HEP.TrkX Project Deep Learning for Particle T.pdf},
  journal = {Journal of Physics: Conference Series}
}


@article{vanroestelZTFSourceClassification2021,
  title = {The {{ZTF Source Classification Project}}. {{I}}. {{Methods}} and {{Infrastructure}}},
  author = {{van Roestel}, Jan and Duev, Dmitry A. and Mahabal, Ashish A. and Coughlin, Michael W. and Mr{\'o}z, Przemek and Burdge, Kevin and Drake, Andrew and Graham, Matthew J. and Hillenbrand, Lynne and Bellm, Eric C. and Kupfer, Thomas and Delacroix, Alexandre and Fremling, C. and Golkhou, V. Zach and Hale, David and Laher, Russ R. and Masci, Frank J. and Riddle, Reed and Rosnet, Philippe and Rusholme, Ben and Smith, Roger and Soumagnac, Maayane T. and Walters, Richard and Prince, Thomas A. and Kulkarni, S. R.},
  year = {2021},
  month = jun,
  journal = {The Astronomical Journal},
  volume = {161},
  number = {6},
  pages = {267},
  issn = {0004-6256, 1538-3881},
  doi = {10.3847/1538-3881/abe853},
  abstract = {Abstract                            The Zwicky Transient Facility (ZTF) has been observing the entire northern sky since the start of 2018 down to a magnitude of 20.5 (5               {$\sigma$}               for 30 s exposure) in the               g               ,               r               , and               i               filters. Over the course of two years, ZTF has obtained light curves of more than a billion sources, each with 50\textendash 1000 epochs per light curve in               g               and               r               , and fewer in               i               . To be able to use the information contained in the light curves of variable sources for new scientific discoveries, an efficient and flexible framework is needed to classify them. In this paper, we introduce the methods and infrastructure that will be used to classify all ZTF light curves. Our approach aims to be flexible and modular and allows the use of a dynamical classification scheme and labels, continuously evolving training sets, and the use of different machine-learning classifier types and architectures. With this setup, we are able to continuously update and improve the classification of ZTF light curves as new data become available, training samples are updated, and new classes need to be incorporated.},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\4YL9PWIJ\\van Roestel et al. - 2021 - The ZTF Source Classification Project. I. Methods .pdf}
}


@article{vasconcellosDecisionTreeClassifiers2010,
  title = {Decision {{Tree Classifiers}} for {{Star}}/{{Galaxy Separation}}},
  author = {Vasconcellos, E. C. and {de Carvalho}, R. R. and Gal, R. R. and LaBarbera, F. L. and Capelato, H. V. and Velho, H. F. Campos and Trevisan, M. and Ruiz, R. S. R.},
  year = {2011},
  month = jun,
  journal = {The Astronomical Journal},
  volume = {141},
  number = {6},
  pages = {189},
  issn = {0004-6256, 1538-3881},
  doi = {10.1088/0004-6256/141/6/189}
}

@article{wuGoogleNeuralMachine2016,
  title = {Google's {{Neural Machine Translation System}}: {{Bridging}} the {{Gap}} between {{Human}} and {{Machine Translation}}},
  shorttitle = {Google's {{Neural Machine Translation System}}},
  author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, {\L}ukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
  year = {2016},
  month = oct,
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1609.08144},
  copyright = {arXiv.org perpetual, non-exclusive license},
  journal = {arXiv:1609.08144},
  eprint = {1609.08144},
  eprinttype = {arxiv},
  primaryclass = {cs},
  language = {en},
}



@article{xuMachineLearningComplex2019,
  title = {Machine Learning and Complex Biological Data},
  author = {Xu, Chunming and Jackson, Scott A.},
  year = {2019},
  month = dec,
  volume = {20},
  pages = {76, s13059-019-1689-0},
  issn = {1474-760X},
  doi = {10.1186/s13059-019-1689-0},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\U5WHFLWA\\Xu and Jackson - 2019 - Machine learning and complex biological data.pdf},
  journal = {Genome Biology},
  language = {en},
  number = {1}
}

@article{yuenDNASequencingQuantum2010,
  title = {{{DNA Sequencing}} via {{Quantum Mechanics}} and {{Machine Learning}}},
  author = {Yuen, Henry and Shimojo, Fuyuki and Zhang, Kevin J. and Nomura, Ken-ichi and Kalia, Rajiv K. and Nakano, Aiichiro and Vashishta, Priya},
  year = {2010},
  month = dec,
  abstract = {Rapid sequencing of individual human genome is prerequisite to genomic medicine, where diseases will be prevented by preemptive cures. Quantum-mechanical tunneling through single-stranded DNA in a solid-state nanopore has been proposed for rapid DNA sequencing, but unfortunately the tunneling current alone cannot distinguish the four nucleotides due to large fluctuations in molecular conformation and solvent. Here, we propose a machine-learning approach applied to the tunneling current-voltage (I-V) characteristic for efficient discrimination between the four nucleotides. We first combine principal component analysis (PCA) and fuzzy c-means (FCM) clustering to learn the "fingerprints" of the electronic density-of-states (DOS) of the four nucleotides, which can be derived from the I-V data. We then apply the hidden Markov model and the Viterbi algorithm to sequence a time series of DOS data (i.e., to solve the sequencing problem). Numerical experiments show that the PCA-FCM approach can classify unlabeled DOS data with 91\% accuracy. Furthermore, the classification is found to be robust against moderate levels of noise, i.e., 70\% accuracy is retained with a signal-to-noise ratio of 26 dB. The PCA-FCM-Viterbi approach provides a 4-fold increase in accuracy for the sequencing problem compared with PCA alone. In conjunction with recent developments in nanotechnology, this machine-learning method may pave the way to the much-awaited rapid, low-cost genome sequencer.},
  archiveprefix = {arXiv},
  eprint = {1012.0900},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\TELN4EWY\\Yuen et al. - 2010 - DNA Sequencing via Quantum Mechanics and Machine L.pdf;C\:\\Users\\sidch\\Zotero\\storage\\EKQVGLBL\\1012.html},
  journal = {arXiv:1012.0900 [physics, q-bio]},
  keywords = {Computer Science - Computational Engineering; Finance; and Science,Physics - Biological Physics,Quantitative Biology - Quantitative Methods},
  primaryclass = {physics, q-bio}
}


@article{lochnerPhotometricSupernovaClassification2016,
  title = {Photometric {{Supernova Classification With Machine Learning}}},
  author = {Lochner, Michelle and McEwen, Jason D. and Peiris, Hiranya V. and Lahav, Ofer and Winter, Max K.},
  year = {2016},
  month = sep,
  doi = {10.3847/0067-0049/225/2/31},
  abstract = {Automated photometric supernova classification has become an active area of research in recent years in light of current and upcoming imaging surveys such as the Dark Energy Survey (DES) and the Large Synoptic Survey Telescope, given that spectroscopic confirmation of type for all supernovae discovered will be impossible. Here, we develop a multi-faceted classification pipeline, combining existing and new approaches. Our pipeline consists of two stages: extracting descriptive features from the light curves and classification using a machine learning algorithm. Our feature extraction methods vary from model-dependent techniques, namely SALT2 fits, to more independent techniques fitting parametric models to curves, to a completely model-independent wavelet approach. We cover a range of representative machine learning algorithms, including naive Bayes, k-nearest neighbors, support vector machines, artificial neural networks and boosted decision trees (BDTs). We test the pipeline on simulated multi-band DES light curves from the Supernova Photometric Classification Challenge. Using the commonly used area under the curve (AUC) of the Receiver Operating Characteristic as a metric, we find that the SALT2 fits and the wavelet approach, with the BDTs algorithm, each achieves an AUC of 0.98, where 1 represents perfect classification. We find that a representative training set is essential for good classification, whatever the feature set or algorithm, with implications for spectroscopic follow-up. Importantly, we find that by using either the SALT2 or the wavelet feature sets with a BDT algorithm, accurate classification is possible purely from light curve data, without the need for any redshift information.},
  archiveprefix = {arXiv},
  eprint = {1603.00882},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\8U8G82Z7\\Lochner et al. - 2016 - Photometric Supernova Classification With Machine .pdf;C\:\\Users\\sidch\\Zotero\\storage\\DWPVT7YG\\1603.html},
  journal = {arXiv:1603.00882 [astro-ph]},
  keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics,Astrophysics - Instrumentation and Methods for Astrophysics},
  primaryclass = {astro-ph}
}


@article{ivezicLSSTScienceDrivers2019,
  title = {{{LSST}}: From {{Science Drivers}} to {{Reference Design}} and {{Anticipated Data Products}}},
  shorttitle = {{{LSST}}},
  author = {Ivezi{\'c}, {\v Z}eljko and Kahn, Steven M. and Tyson, J. Anthony and Abel, Bob and Acosta, Emily and Allsman, Robyn and Alonso, David and AlSayyad, Yusra and Anderson, Scott F. and Andrew, John and Angel, James Roger P. and Angeli, George Z. and Ansari, Reza and Antilogus, Pierre and Araujo, Constanza and Armstrong, Robert and Arndt, Kirk T. and Astier, Pierre and Aubourg, {\'E}ric and Auza, Nicole and Axelrod, Tim S. and Bard, Deborah J. and Barr, Jeff D. and Barrau, Aurelian and Bartlett, James G. and Bauer, Amanda E. and Bauman, Brian J. and Baumont, Sylvain and Becker, Andrew C. and Becla, Jacek and Beldica, Cristina and Bellavia, Steve and Bianco, Federica B. and Biswas, Rahul and Blanc, Guillaume and Blazek, Jonathan and Blandford, Roger D. and Bloom, Josh S. and Bogart, Joanne and Bond, Tim W. and Borgland, Anders W. and Borne, Kirk and Bosch, James F. and Boutigny, Dominique and Brackett, Craig A. and Bradshaw, Andrew and Brandt, William Nielsen and Brown, Michael E. and Bullock, James S. and Burchat, Patricia and Burke, David L. and Cagnoli, Gianpietro and Calabrese, Daniel and Callahan, Shawn and Callen, Alice L. and Chandrasekharan, Srinivasan and {Charles-Emerson}, Glenaver and Chesley, Steve and Cheu, Elliott C. and Chiang, Hsin-Fang and Chiang, James and Chirino, Carol and Chow, Derek and Ciardi, David R. and Claver, Charles F. and {Cohen-Tanugi}, Johann and Cockrum, Joseph J. and Coles, Rebecca and Connolly, Andrew J. and Cook, Kem H. and Cooray, Asantha and Covey, Kevin R. and Cribbs, Chris and Cui, Wei and Cutri, Roc and Daly, Philip N. and Daniel, Scott F. and Daruich, Felipe and Daubard, Guillaume and Daues, Greg and Dawson, William and Delgado, Francisco and Dellapenna, Alfred and {de Peyster}, Robert and {de Val-Borro}, Miguel and Digel, Seth W. and Doherty, Peter and Dubois, Richard and {Dubois-Felsmann}, Gregory P. and Durech, Josef and Economou, Frossie and Eracleous, Michael and Ferguson, Henry and Figueroa, Enrique and {Fisher-Levine}, Merlin and Focke, Warren and Foss, Michael D. and Frank, James and Freemon, Michael D. and Gangler, Emmanuel and Gawiser, Eric and Geary, John C. and Gee, Perry and Geha, Marla and Gessner, Charles J. B. and Gibson, Robert R. and Gilmore, D. Kirk and Glanzman, Thomas and Glick, William and Goldina, Tatiana and Goldstein, Daniel A. and Goodenow, Iain and Graham, Melissa L. and Gressler, William J. and Gris, Philippe and Guy, Leanne P. and Guyonnet, Augustin and Haller, Gunther and Harris, Ron and Hascall, Patrick A. and Haupt, Justine and Hernandez, Fabio and Herrmann, Sven and Hileman, Edward and Hoblitt, Joshua and Hodgson, John A. and Hogan, Craig and Huang, Dajun and Huffer, Michael E. and Ingraham, Patrick and Innes, Walter R. and Jacoby, Suzanne H. and Jain, Bhuvnesh and Jammes, Fabrice and Jee, James and Jenness, Tim and Jernigan, Garrett and Jevremovi{\'c}, Darko and Johns, Kenneth and Johnson, Anthony S. and Johnson, Margaret W. G. and Jones, R. Lynne and {Juramy-Gilles}, Claire and Juri{\'c}, Mario and Kalirai, Jason S. and Kallivayalil, Nitya J. and Kalmbach, Bryce and Kantor, Jeffrey P. and Karst, Pierre and Kasliwal, Mansi M. and Kelly, Heather and Kessler, Richard and Kinnison, Veronica and Kirkby, David and Knox, Lloyd and Kotov, Ivan V. and Krabbendam, Victor L. and Krughoff, K. Simon and Kub{\'a}nek, Petr and Kuczewski, John and Kulkarni, Shri and Ku, John and Kurita, Nadine R. and Lage, Craig S. and Lambert, Ron and Lange, Travis and Langton, J. Brian and Guillou, Laurent Le and Levine, Deborah and Liang, Ming and Lim, Kian-Tat and Lintott, Chris J. and Long, Kevin E. and Lopez, Margaux and Lotz, Paul J. and Lupton, Robert H. and Lust, Nate B. and MacArthur, Lauren A. and Mahabal, Ashish and Mandelbaum, Rachel and Marsh, Darren S. and Marshall, Philip J. and Marshall, Stuart and May, Morgan and McKercher, Robert and McQueen, Michelle and Meyers, Joshua and Migliore, Myriam and Miller, Michelle and Mills, David J. and Miraval, Connor and Moeyens, Joachim and Monet, David G. and Moniez, Marc and Monkewitz, Serge and Montgomery, Christopher and Mueller, Fritz and Muller, Gary P. and Arancibia, Freddy Mu{\~n}oz and Neill, Douglas R. and Newbry, Scott P. and Nief, Jean-Yves and Nomerotski, Andrei and Nordby, Martin and O'Connor, Paul and Oliver, John and Olivier, Scot S. and Olsen, Knut and O'Mullane, William and Ortiz, Sandra and Osier, Shawn and Owen, Russell E. and Pain, Reynald and Palecek, Paul E. and Parejko, John K. and Parsons, James B. and Pease, Nathan M. and Peterson, J. Matt and Peterson, John R. and Petravick, Donald L. and Petrick, M. E. Libby and Petry, Cathy E. and Pierfederici, Francesco and Pietrowicz, Stephen and Pike, Rob and Pinto, Philip A. and Plante, Raymond and Plate, Stephen and Price, Paul A. and Prouza, Michael and Radeka, Veljko and Rajagopal, Jayadev and Rasmussen, Andrew P. and Regnault, Nicolas and Reil, Kevin A. and Reiss, David J. and Reuter, Michael A. and Ridgway, Stephen T. and Riot, Vincent J. and Ritz, Steve and Robinson, Sean and Roby, William and Roodman, Aaron and Rosing, Wayne and Roucelle, Cecille and Rumore, Matthew R. and Russo, Stefano and Saha, Abhijit and Sassolas, Benoit and Schalk, Terry L. and Schellart, Pim and Schindler, Rafe H. and Schmidt, Samuel and Schneider, Donald P. and Schneider, Michael D. and Schoening, William and Schumacher, German and Schwamb, Megan E. and Sebag, Jacques and Selvy, Brian and Sembroski, Glenn H. and Seppala, Lynn G. and Serio, Andrew and Serrano, Eduardo and Shaw, Richard A. and Shipsey, Ian and Sick, Jonathan and Silvestri, Nicole and Slater, Colin T. and Smith, J. Allyn and Smith, R. Chris and Sobhani, Shahram and Soldahl, Christine and {Storrie-Lombardi}, Lisa and Stover, Edward and Strauss, Michael A. and Street, Rachel A. and Stubbs, Christopher W. and Sullivan, Ian S. and Sweeney, Donald and Swinbank, John D. and Szalay, Alexander and Takacs, Peter and Tether, Stephen A. and Thaler, Jon J. and Thayer, John Gregg and Thomas, Sandrine and Thukral, Vaikunth and Tice, Jeffrey and Trilling, David E. and Turri, Max and Van Berg, Richard and Berk, Daniel Vanden and Vetter, Kurt and Virieux, Francoise and Vucina, Tomislav and Wahl, William and Walkowicz, Lucianne and Walsh, Brian and Walter, Christopher W. and Wang, Daniel L. and Wang, Shin-Yawn and Warner, Michael and Wiecha, Oliver and Willman, Beth and Winters, Scott E. and Wittman, David and Wolff, Sidney C. and {Wood-Vasey}, W. Michael and Wu, Xiuqin and Xin, Bo and Yoachim, Peter and Zhan, Hu},
  year = {2019},
  month = mar,
  volume = {873},
  pages = {111},
  issn = {1538-4357},
  doi = {10.3847/1538-4357/ab042c},
  abstract = {(Abridged) We describe here the most ambitious survey currently planned in the optical, the Large Synoptic Survey Telescope (LSST). A vast array of science will be enabled by a single wide-deep-fast sky survey, and LSST will have unique survey capability in the faint time domain. The LSST design is driven by four main science themes: probing dark energy and dark matter, taking an inventory of the Solar System, exploring the transient optical sky, and mapping the Milky Way. LSST will be a wide-field ground-based system sited at Cerro Pach\textbackslash '\{o\}n in northern Chile. The telescope will have an 8.4 m (6.5 m effective) primary mirror, a 9.6 deg\$\^2\$ field of view, and a 3.2 Gigapixel camera. The standard observing sequence will consist of pairs of 15-second exposures in a given field, with two such visits in each pointing in a given night. With these repeats, the LSST system is capable of imaging about 10,000 square degrees of sky in a single filter in three nights. The typical 5\$\textbackslash sigma\$ point-source depth in a single visit in \$r\$ will be \$\textbackslash sim 24.5\$ (AB). The project is in the construction phase and will begin regular survey operations by 2022. The survey area will be contained within 30,000 deg\$\^2\$ with \$\textbackslash delta{$<$}+34.5\^\textbackslash circ\$, and will be imaged multiple times in six bands, \$ugrizy\$, covering the wavelength range 320--1050 nm. About 90\textbackslash\% of the observing time will be devoted to a deep-wide-fast survey mode which will uniformly observe a 18,000 deg\$\^2\$ region about 800 times (summed over all six bands) during the anticipated 10 years of operations, and yield a coadded map to \$r\textbackslash sim27.5\$. The remaining 10\textbackslash\% of the observing time will be allocated to projects such as a Very Deep and Fast time domain survey. The goal is to make LSST data products, including a relational database of about 32 trillion observations of 40 billion objects, available to the public and scientists around the world.},
  archiveprefix = {arXiv},
  eprint = {0805.2366},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\NYALM3GL\\Iveziƒá et al. - 2019 - LSST from Science Drivers to Reference Design and.pdf;C\:\\Users\\sidch\\Zotero\\storage\\Y4UB8WLK\\0805.html},
  journal = {The Astrophysical Journal},
  keywords = {Astrophysics},
  number = {2}
}


@book{bishopPatternRecognitionMachine2006,
  title = {Pattern Recognition and Machine Learning},
  author = {Bishop, Christopher M.},
  year = {2006},
  publisher = {{Springer}},
  address = {{New York}},
  isbn = {978-0-387-31073-2},
  keywords = {Machine learning,Pattern perception},
  lccn = {Q327 .B52 2006},
  series = {Information Science and Statistics}
}


@article{Kalyanam2017,
  title = {Machine Learning and Applications on Social Media Data},
  author = {Kalyanam, Janani and Lanckriet, Gert},
  year = {2017},
  journal = {UC San Diego, Retrieved from \href{https://escholarship.org/uc/item/6545w71z}{https://escholarship.org/uc/item/6545w71z}},
  language = {en}
}


@article{breimanRandomForests2001,
  title = {Random {{Forests}}},
  author = {Breiman, Leo},
  year = {2001},
  volume = {45},
  pages = {5--32},
  issn = {08856125},
  doi = {10.1023/A:1010933404324},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\ZMZ4PZGV\\Breiman - 2001 - [No title found].pdf},
  journal = {Machine Learning},
  number = {1}
}

@article{chenXGBoostScalableTree2016,
  title = {{{XGBoost}}: {{A Scalable Tree Boosting System}}},
  shorttitle = {{{XGBoost}}},
  author = {Chen, Tianqi and Guestrin, Carlos},
  year = {2016},
  month = aug,
  pages = {785--794},
  doi = {10.1145/2939672.2939785},
  abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
  archiveprefix = {arXiv},
  eprint = {1603.02754},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\RLFPIPFS\\Chen and Guestrin - 2016 - XGBoost A Scalable Tree Boosting System.pdf;C\:\\Users\\sidch\\Zotero\\storage\\LA2AKAA9\\1603.html},
  journal = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  keywords = {Computer Science - Machine Learning}
}


@article{chenZwickyTransientFacility2020a,
  title = {The {{Zwicky Transient Facility Catalog}} of {{Periodic Variable Stars}}},
  author = {Chen, Xiaodian and Wang, Shu and Deng, Licai and {de Grijs}, Richard and Yang, Ming and Tian, Hao},
  year = {2020},
  month = jul,
  journal = {The Astrophysical Journal Supplement Series},
  volume = {249},
  number = {1},
  pages = {18},
  issn = {1538-4365},
  doi = {10.3847/1538-4365/ab9cae},
  keywords = {Astrophysics - Astrophysics of Galaxies,Astrophysics - Solar and Stellar Astrophysics},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\G27MYJ6C\\Chen et al. - 2020 - The Zwicky Transient Facility Catalog of Periodic .pdf;C\:\\Users\\sidch\\Zotero\\storage\\JC9MS2TH\\Chen et al. - 2020 - The Zwicky Transient Facility Catalog of Periodic .pdf;C\:\\Users\\sidch\\Zotero\\storage\\KVDCDLLL\\Chen et al. - 2020 - The Zwicky Transient Facility Catalog of Periodic .pdf;C\:\\Users\\sidch\\Zotero\\storage\\RL4IA55C\\Chen et al. - 2020 - The Zwicky Transient Facility Catalog of Periodic .pdf;C\:\\Users\\sidch\\Zotero\\storage\\VG44KDAQ\\2005.html}
}


@inproceedings{sanchez2020alert,
  title={Alert Classification for the ALeRCE Broker System: The Light Curve Classifier},
  author={S{\'a}nchez-S{\'a}ez, P and Reyes, I and Valenzuela, C and F{\"o}rster, F and Eyheramendy, S and Elorrieta, F and Bauer, FE and Cabrera-Vives, G and Est{\'e}vez, PA and Catelan, M and others},
  year={2020}
}


@article{sanchez-saezAlertClassificationALeRCE2021,
  title = {Alert {{Classification}} for the {{ALeRCE Broker System}}: The {{Light Curve Classifier}}},
  shorttitle = {Alert {{Classification}} for the {{ALeRCE Broker System}}},
  author = {{S{\'a}nchez-S{\'a}ez}, P. and Reyes, I. and Valenzuela, C. and F{\"o}rster, F. and Eyheramendy, S. and Elorrieta, F. and Bauer, F. E. and {Cabrera-Vives}, G. and Est{\'e}vez, P. A. and Catelan, M. and Pignata, G. and Huijse, P. and De Cicco, D. and Ar{\'e}valo, P. and {Carrasco-Davis}, R. and Abril, J. and Kurtev, R. and Borissova, J. and Arredondo, J. and {Castillo-Navarrete}, E. and Rodriguez, D. and {Ruz-Mieres}, D. and Moya, A. and {Sabatini-Gacit{\'u}a}, L. and {Sep{\'u}lveda-Cobo}, C. and {Camacho-I{\~n}iguez}, E.},
  year = {2021},
  month = mar,
  journal = {The Astronomical Journal},
  volume = {161},
  number = {3},
  pages = {141},
  issn = {0004-6256, 1538-3881},
  doi = {10.3847/1538-3881/abd5c1},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\NBMY33GX\\S√°nchez-S√°ez et al. - 2021 - Alert Classification for the ALeRCE Broker System.pdf}
}


@book{lsstsciencecollaborationLSSTScienceBook2009,
  title = {{{LSST Science Book}}, {{Version}} 2.0},
  author = {{LSST Science Collaboration} and Abell, Paul A. and Allison, Julius and Anderson, Scott F. and Andrew, John R. and Angel, J. Roger P. and Armus, Lee and Arnett, David and Asztalos, S. J. and Axelrod, Tim S. and Bailey, Stephen and Ballantyne, D. R. and Bankert, Justin R. and Barkhouse, Wayne A. and Barr, Jeffrey D. and Barrientos, L. Felipe and Barth, Aaron J. and Bartlett, James G. and Becker, Andrew C. and Becla, Jacek and Beers, Timothy C. and Bernstein, Joseph P. and Biswas, Rahul and Blanton, Michael R. and Bloom, Joshua S. and Bochanski, John J. and Boeshaar, Pat and Borne, Kirk D. and Bradac, Marusa and Brandt, W. N. and Bridge, Carrie R. and Brown, Michael E. and Brunner, Robert J. and Bullock, James S. and Burgasser, Adam J. and Burge, James H. and Burke, David L. and Cargile, Phillip A. and Chandrasekharan, Srinivasan and Chartas, George and Chesley, Steven R. and Chu, You-Hua and Cinabro, David and Claire, Mark W. and Claver, Charles F. and Clowe, Douglas and Connolly, A. J. and Cook, Kem H. and Cooke, Jeff and Cooray, Asantha and Covey, Kevin R. and Culliton, Christopher S. and {de Jong}, Roelof and {de Vries}, Willem H. and Debattista, Victor P. and Delgado, Francisco and Dell'Antonio, Ian P. and Dhital, Saurav and Di Stefano, Rosanne and Dickinson, Mark and Dilday, Benjamin and Djorgovski, S. G. and Dobler, Gregory and Donalek, Ciro and {Dubois-Felsmann}, Gregory and Durech, Josef and Eliasdottir, Ardis and Eracleous, Michael and Eyer, Laurent and Falco, Emilio E. and Fan, Xiaohui and Fassnacht, Christopher D. and Ferguson, Harry C. and Fernandez, Yanga R. and Fields, Brian D. and Finkbeiner, Douglas and Figueroa, Eduardo E. and Fox, Derek B. and Francke, Harold and Frank, James S. and Frieman, Josh and Fromenteau, Sebastien and Furqan, Muhammad and Galaz, Gaspar and {Gal-Yam}, A. and Garnavich, Peter and Gawiser, Eric and Geary, John and Gee, Perry and Gibson, Robert R. and Gilmore, Kirk and Grace, Emily A. and Green, Richard F. and Gressler, William J. and Grillmair, Carl J. and Habib, Salman and Haggerty, J. S. and Hamuy, Mario and Harris, Alan W. and Hawley, Suzanne L. and Heavens, Alan F. and Hebb, Leslie and Henry, Todd J. and Hileman, Edward and Hilton, Eric J. and Hoadley, Keri and Holberg, J. B. and Holman, Matt J. and Howell, Steve B. and Infante, Leopoldo and Ivezic, Zeljko and Jacoby, Suzanne H. and Jain, Bhuvnesh and R and Jedicke and Jee, M. James and Jernigan, J. Garrett and Jha, Saurabh W. and Johnston, Kathryn V. and Jones, R. Lynne and Juric, Mario and Kaasalainen, Mikko and Styliani and Kafka and Kahn, Steven M. and Kaib, Nathan A. and Kalirai, Jason and Kantor, Jeff and Kasliwal, Mansi M. and Keeton, Charles R. and Kessler, Richard and Knezevic, Zoran and Kowalski, Adam and Krabbendam, Victor L. and Krughoff, K. Simon and Kulkarni, Shrinivas and Kuhlman, Stephen and Lacy, Mark and Lepine, Sebastien and Liang, Ming and Lien, Amy and Lira, Paulina and Long, Knox S. and Lorenz, Suzanne and Lotz, Jennifer M. and Lupton, R. H. and Lutz, Julie and Macri, Lucas M. and Mahabal, Ashish A. and Mandelbaum, Rachel and Marshall, Phil and May, Morgan and McGehee, Peregrine M. and Meadows, Brian T. and Meert, Alan and Milani, Andrea and Miller, Christopher J. and Miller, Michelle and Mills, David and Minniti, Dante and Monet, David and Mukadam, Anjum S. and Nakar, Ehud and Neill, Douglas R. and Newman, Jeffrey A. and Nikolaev, Sergei and Nordby, Martin and O'Connor, Paul and Oguri, Masamune and Oliver, John and Olivier, Scot S. and Olsen, Julia K. and Olsen, Knut and Olszewski, Edward W. and Oluseyi, Hakeem and Padilla, Nelson D. and Parker, Alex and Pepper, Joshua and Peterson, John R. and Petry, Catherine and Pinto, Philip A. and Pizagno, James L. and Popescu, Bogdan and Prsa, Andrej and Radcka, Veljko and Raddick, M. Jordan and Rasmussen, Andrew and Rau, Arne and Rho, Jeonghee and Rhoads, James E. and Richards, Gordon T. and Ridgway, Stephen T. and Robertson, Brant E. and Roskar, Rok and Saha, Abhijit and Sarajedini, Ata and Scannapieco, Evan and Schalk, Terry and Schindler, Rafe and Schmidt, Samuel and Schmidt, Sarah and Schneider, Donald P. and Schumacher, German and Scranton, Ryan and Sebag, Jacques and Seppala, Lynn G. and Shemmer, Ohad and Simon, Joshua D. and Sivertz, M. and Smith, Howard A. and Smith, J. Allyn and Smith, Nathan and Spitz, Anna H. and Stanford, Adam and Stassun, Keivan G. and Strader, Jay and Strauss, Michael A. and Stubbs, Christopher W. and Sweeney, Donald W. and Szalay, Alex and Szkody, Paula and Takada, Masahiro and Thorman, Paul and Trilling, David E. and Trimble, Virginia and Tyson, Anthony and Van Berg, Richard and Berk, Daniel Vanden and VanderPlas, Jake and Verde, Licia and Vrsnak, Bojan and Walkowicz, Lucianne M. and Wandelt, Benjamin D. and Wang, Sheng and Wang, Yun and Warner, Michael and Wechsler, Risa H. and West, Andrew A. and Wiecha, Oliver and Williams, Benjamin F. and Willman, Beth and Wittman, David and Wolff, Sidney C. and {Wood-Vasey}, W. Michael and Wozniak, Przemek and Young, Patrick and Zentner, Andrew and Zhan, Hu},
  year = {2009},
  publisher = {LSST Corporation},
  month = dec,
  eprint = {0912.0201},
  eprinttype = {arxiv},
  abstract = {A survey that can cover the sky in optical bands over wide fields to faint magnitudes with a fast cadence will enable many of the exciting science opportunities of the next decade. The Large Synoptic Survey Telescope (LSST) will have an effective aperture of 6.7 meters and an imaging camera with field of view of 9.6 deg\^2, and will be devoted to a ten-year imaging survey over 20,000 deg\^2 south of +15 deg. Each pointing will be imaged 2000 times with fifteen second exposures in six broad bands from 0.35 to 1.1 microns, to a total point-source depth of r\textasciitilde 27.5. The LSST Science Book describes the basic parameters of the LSST hardware, software, and observing plans. The book discusses educational and outreach opportunities, then goes on to describe a broad range of science that LSST will revolutionize: mapping the inner and outer Solar System, stellar populations in the Milky Way and nearby galaxies, the structure of the Milky Way disk and halo and other objects in the Local Volume, transient and variable objects both at low and high redshift, and the properties of normal and active galaxies at low and high redshift. It then turns to far-field cosmological topics, exploring properties of supernovae to z\textasciitilde 1, strong and weak lensing, the large-scale distribution of galaxies and baryon oscillations, and how these different probes may be combined to constrain cosmological models and the physics of dark energy.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - Astrophysics of Galaxies,Astrophysics - Cosmology and Nongalactic Astrophysics,Astrophysics - Earth and Planetary Astrophysics,Astrophysics - Instrumentation and Methods for Astrophysics,Astrophysics - Solar and Stellar Astrophysics},
  file = {C\:\\Users\\sidch\\Zotero\\storage\\Y99ARYBJ\\LSST Science Collaboration et al. - 2009 - LSST Science Book, Version 2.0.pdf;C\:\\Users\\sidch\\Zotero\\storage\\6SDIXIJX\\0912.html}
}


@article{yorkSloanDigitalSky2000,
       author = {{York}, Donald G. and {Adelman}, J. and {Anderson}, John E., Jr. and {Anderson}, Scott F. and {Annis}, James and {Bahcall}, Neta A. and {Bakken}, J.~A. and {Barkhouser}, Robert and {Bastian}, Steven and {Berman}, Eileen and {Boroski}, William N. and {Bracker}, Steve and {Briegel}, Charlie and {Briggs}, John W. and {Brinkmann}, J. and {Brunner}, Robert and {Burles}, Scott and {Carey}, Larry and {Carr}, Michael A. and {Castander}, Francisco J. and {Chen}, Bing and {Colestock}, Patrick L. and {Connolly}, A.~J. and {Crocker}, J.~H. and {Csabai}, Istv{\'a}n and {Czarapata}, Paul C. and {Davis}, John Eric and {Doi}, Mamoru and {Dombeck}, Tom and {Eisenstein}, Daniel and {Ellman}, Nancy and {Elms}, Brian R. and {Evans}, Michael L. and {Fan}, Xiaohui and {Federwitz}, Glenn R. and {Fiscelli}, Larry and {Friedman}, Scott and {Frieman}, Joshua A. and {Fukugita}, Masataka and {Gillespie}, Bruce and {Gunn}, James E. and {Gurbani}, Vijay K. and {de Haas}, Ernst and {Haldeman}, Merle and {Harris}, Frederick H. and {Hayes}, J. and {Heckman}, Timothy M. and {Hennessy}, G.~S. and {Hindsley}, Robert B. and {Holm}, Scott and {Holmgren}, Donald J. and {Huang}, Chi-hao and {Hull}, Charles and {Husby}, Don and {Ichikawa}, Shin-Ichi and {Ichikawa}, Takashi and {Ivezi{\'c}}, {\v{Z}}eljko and {Kent}, Stephen and {Kim}, Rita S.~J. and {Kinney}, E. and {Klaene}, Mark and {Kleinman}, A.~N. and {Kleinman}, S. and {Knapp}, G.~R. and {Korienek}, John and {Kron}, Richard G. and {Kunszt}, Peter Z. and {Lamb}, D.~Q. and {Lee}, B. and {Leger}, R. French and {Limmongkol}, Siriluk and {Lindenmeyer}, Carl and {Long}, Daniel C. and {Loomis}, Craig and {Loveday}, Jon and {Lucinio}, Rich and {Lupton}, Robert H. and {MacKinnon}, Bryan and {Mannery}, Edward J. and {Mantsch}, P.~M. and {Margon}, Bruce and {McGehee}, Peregrine and {McKay}, Timothy A. and {Meiksin}, Avery and {Merelli}, Aronne and {Monet}, David G. and {Munn}, Jeffrey A. and {Narayanan}, Vijay K. and {Nash}, Thomas and {Neilsen}, Eric and {Neswold}, Rich and {Newberg}, Heidi Jo and {Nichol}, R.~C. and {Nicinski}, Tom and {Nonino}, Mario and {Okada}, Norio and {Okamura}, Sadanori and {Ostriker}, Jeremiah P. and {Owen}, Russell and {Pauls}, A. George and {Peoples}, John and {Peterson}, R.~L. and {Petravick}, Donald and {Pier}, Jeffrey R. and {Pope}, Adrian and {Pordes}, Ruth and {Prosapio}, Angela and {Rechenmacher}, Ron and {Quinn}, Thomas R. and {Richards}, Gordon T. and {Richmond}, Michael W. and {Rivetta}, Claudio H. and {Rockosi}, Constance M. and {Ruthmansdorfer}, Kurt and {Sandford}, Dale and {Schlegel}, David J. and {Schneider}, Donald P. and {Sekiguchi}, Maki and {Sergey}, Gary and {Shimasaku}, Kazuhiro and {Siegmund}, Walter A. and {Smee}, Stephen and {Smith}, J. Allyn and {Snedden}, S. and {Stone}, R. and {Stoughton}, Chris and {Strauss}, Michael A. and {Stubbs}, Christopher and {SubbaRao}, Mark and {Szalay}, Alexander S. and {Szapudi}, Istvan and {Szokoly}, Gyula P. and {Thakar}, Anirudda R. and {Tremonti}, Christy and {Tucker}, Douglas L. and {Uomoto}, Alan and {Vanden Berk}, Dan and {Vogeley}, Michael S. and {Waddell}, Patrick and {Wang}, Shu-i. and {Watanabe}, Masaru and {Weinberg}, David H. and {Yanny}, Brian and {Yasuda}, Naoki and {SDSS Collaboration}},
        title = "{The Sloan Digital Sky Survey: Technical Summary}",
      journal = {\aj},
     keywords = {Cosmology: Observations, Instrumentation: Miscellaneous, Astrophysics},
         year = 2000,
        month = sep,
       volume = {120},
       number = {3},
        pages = {1579-1587},
          doi = {10.1086/301513},
archivePrefix = {arXiv},
       eprint = {astro-ph/0006396},
 primaryClass = {astro-ph},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2000AJ....120.1579Y},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@phdthesis{mcwhirterAutomatedPipelineVariability2018,
  title = {An {{Automated Pipeline}} for {{Variability Detection}} and {{Classification}} for the {{Small Telescopes Installed}} at the {{Liverpool Telescope}}},
  author = {McWhirter, PR},
  year = {2018},
  school = {Liverpool John Moores University},
  url = {http://researchonline.ljmu.ac.uk/id/eprint/9479}
}

@phdthesis{millerTimeDomainStudiesProbe2013,
  title = {Time-{{Domain Studies}} as a {{Probe}} of {{Stellar Evolution}}},
  author = {Miller, Adam Andrew},
  year = {2013},
  langid = {english},
  school = {University of California, Berkeley},
  url = {https://escholarship.org/uc/item/2mm8338z}
}


@phdthesis{villarTimeDomainStudiesNew2020,
  title = {Time-{{Domain Studies}} in the {{New Eras}} of {{Multi-Messenger Astrophysics}} and {{Big Data}}},
  author = {Villar, Victoria Ashley},
  year = {2020},
  month = may,
  school = {Harvard University},
  url = {https://nrs.harvard.edu/URN-3:HUL.INSTREPOS:37365823}
}


@inproceedings{sklearn_api,
  author    = {Lars Buitinck and Gilles Louppe and Mathieu Blondel and
  Fabian Pedregosa and Andreas Mueller and Olivier Grisel and
  Vlad Niculae and Peter Prettenhofer and Alexandre Gramfort
  and Jaques Grobler and Robert Layton and Jake VanderPlas and
  Arnaud Joly and Brian Holt and Ga{\"{e}}l Varoquaux},
  title     = {{API} design for machine learning software: experiences from the scikit-learn project},
  booktitle = {ECML PKDD Workshop: Languages for Data Mining and Machine Learning},
  year      = {2013},
  pages = {108--122},
}

@article{scikit-learn,
  title={Scikit-learn: Machine Learning in {P}ython},
  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
  and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
  and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
  Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal={Journal of Machine Learning Research},
  volume={12},
  pages={2825--2830},
  year={2011}
}

@inproceedings{CrossValidation95,
author = {Kohavi, Ron},
title = {A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection},
year = {1995},
isbn = {1558603638},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We review accuracy estimation methods and compare the two most common methods crossvalidation and bootstrap. Recent experimental results on artificial data and theoretical re cults in restricted settings have shown that for selecting a good classifier from a set of classifiers (model selection), ten-fold cross-validation may be better than the more expensive leaveone-out cross-validation. We report on a largescale experiment--over half a million runs of C4.5 and a Naive-Bayes algorithm--to estimate the effects of different parameters on these algrithms on real-world datasets. For crossvalidation we vary the number of folds and whether the folds are stratified or not, for bootstrap, we vary the number of bootstrap samples. Our results indicate that for real-word datasets similar to ours, The best method to use for model selection is ten fold stratified cross validation even if computation power allows using more folds.},
booktitle = {Proceedings of the 14th International Joint Conference on Artificial Intelligence - Volume 2},
pages = {1137‚Äì1143},
numpages = {7},
location = {Montreal, Quebec, Canada},
series = {IJCAI'95}
}